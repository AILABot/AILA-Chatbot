{"docstore/metadata": {"c8f3d9f9-bd00-49af-9c33-34dbdb9fc164": {"doc_hash": "e722408823b732039f6e123598054d6921a1affb83127c3ddb9dce6d87e629c5"}, "e6f11682-d4fb-447b-a354-8dc12d52df1c": {"doc_hash": "2c4f4228fd3bebbb2000c81c66ce7292f78b8a65061507afdf11c5251f461e08"}, "275c23d1-c723-4cf2-baf0-569475b091c6": {"doc_hash": "a9a9ccfc90e8b4b86c762be4ca94e6353a8832c3cc535040011d50f6a9e36d41"}, "d9a8c255-934f-411e-9a1b-008a582f1d79": {"doc_hash": "0ec5e9f2418528a9a0c9881e05f3b2b5a68d29b52c0e2f0442e15b51a4fd4e28"}, "b10cd7bc-9eee-45a7-9bfd-34dfc6eeaaef": {"doc_hash": "c68de3cd673b554dafd5a0c3ab8969d3e4046cea2637df9f756910fb78e6bde3"}, "9e7939ec-4b42-4798-9004-3c4e484a0736": {"doc_hash": "2d92a9e7f1ae0991276e03634372dcf5d1ea7bac71723990b09fbad498bfc2b3"}, "9da02523-aee4-474a-9216-df88f7e18304": {"doc_hash": "5db10ae54641c9ee700f9430f18945999872a1cec2b8a54bcc2fcdf6e1006b1c"}, "b2649ba2-dd10-48b4-9d40-ad82e540cb0d": {"doc_hash": "60e7eb8b3013bd83dfc657a7b0bb30ca0c7fdb2df68f2bf26599642d4c4fbb3c"}, "c5bbfdfb-e5c9-4f0c-a2ea-7cbf685e064a": {"doc_hash": "b9f52ee859dd4aa8ec453b245c353cdf4b5d7b08b10b805307db8bd56c67ae3c"}, "b0e71925-7911-4bf4-8add-e7cb4982922a": {"doc_hash": "67ceee059b1be00e0ace3a315b612ee34281bc5e025f6c03cc9e4b2302fb7c82"}, "0afa5d2b-eb61-44be-bab9-cbf5fec37f75": {"doc_hash": "e2d250d7b3f0fd3b3a5e2d64d3aa795812be9cf4f147d442ef76fe5419164f5a"}, "8178b0ca-56f1-482d-a155-d738b208cbfd": {"doc_hash": "761e29a06720e29595cd89591b127ac9cfab8b68ab18fa69409961647b2f6828"}, "59b2c013-b14a-405a-bf6e-33b30869c76c": {"doc_hash": "42fc86baeeab0d450bc3fd044e00d152542dfd1663231e60120c6505c3158a1d"}, "34c90045-8f90-4fc1-8151-6a45be78a89d": {"doc_hash": "a58f32f554b8b47a718f3231b9e2f6a584dfbdd2f37c63ca498fee4624ddbc82"}, "ab8d5641-227d-49e5-876d-9eeebc047bb3": {"doc_hash": "eedd110535ecdbdf93befdcfc811fa55d7125006348aa48a258fbfc29bd9fe88"}, "e600e4dc-be9e-4b52-8247-6536f10a76a1": {"doc_hash": "27cf414f7c73ab11cfb763850ea2b19ba35f7d0a327ffc049da2bc40b6dff2bb"}, "962bde0e-2bf5-4951-8671-25a5e5f98234": {"doc_hash": "93c4d07128710354dcd57429b829091c0173605e70616352144f85096440625d"}, "198ffda9-909e-4e53-ac6a-656a2463e048": {"doc_hash": "62c66717b5b75a377ba40bc69e169b21af7737d7893386698a8a28d150c78044"}, "a3598513-f8c2-42b1-bae8-60d457cd13ef": {"doc_hash": "c376b0f042cc89ce677b4dd016c2b6ca3722f6c67d8f3cfee21f543546190159"}, "47febcaa-c535-403d-ae2c-a7e5818288f4": {"doc_hash": "f3507e7c8285e100619cbd473379bd6fcbe3866b7fa2f8bdb547530eb6ffbdaf"}, "30b14f95-3f04-4b25-882d-377f0a409946": {"doc_hash": "0ec5e9f2418528a9a0c9881e05f3b2b5a68d29b52c0e2f0442e15b51a4fd4e28"}, "3d96794d-9715-4b05-af57-8f0bb9887cca": {"doc_hash": "c68de3cd673b554dafd5a0c3ab8969d3e4046cea2637df9f756910fb78e6bde3"}, "74263b98-7f95-4891-a063-a3d61dc21e4e": {"doc_hash": "2d92a9e7f1ae0991276e03634372dcf5d1ea7bac71723990b09fbad498bfc2b3"}, "0dea7eec-bf13-433e-bed3-f1d1d71670a6": {"doc_hash": "5db10ae54641c9ee700f9430f18945999872a1cec2b8a54bcc2fcdf6e1006b1c"}, "409a979f-7f4b-4c47-869d-c4c32f5ee14f": {"doc_hash": "60e7eb8b3013bd83dfc657a7b0bb30ca0c7fdb2df68f2bf26599642d4c4fbb3c"}, "e6e0a46e-ac13-4ccd-bf95-c59e002bc570": {"doc_hash": "b9f52ee859dd4aa8ec453b245c353cdf4b5d7b08b10b805307db8bd56c67ae3c"}, "6498b211-a87c-445d-9f6f-7d97469b1bb4": {"doc_hash": "67ceee059b1be00e0ace3a315b612ee34281bc5e025f6c03cc9e4b2302fb7c82"}, "5878b409-641a-459e-93df-f2b0da06c6d6": {"doc_hash": "e2d250d7b3f0fd3b3a5e2d64d3aa795812be9cf4f147d442ef76fe5419164f5a"}, "e62eccb5-333d-459c-90bc-dfa6f1789c1a": {"doc_hash": "968bad5299ba141e01344e2606cd6160472748599c243ba39d30f99a0c3eb245"}, "86d35ee7-a9e0-4e84-8f1d-03285f519f19": {"doc_hash": "a58f32f554b8b47a718f3231b9e2f6a584dfbdd2f37c63ca498fee4624ddbc82"}, "a9959a8a-b143-47fd-8a92-8509de87b055": {"doc_hash": "eedd110535ecdbdf93befdcfc811fa55d7125006348aa48a258fbfc29bd9fe88"}, "b3063718-9644-472b-8f59-d7dd1b67ef37": {"doc_hash": "27cf414f7c73ab11cfb763850ea2b19ba35f7d0a327ffc049da2bc40b6dff2bb"}, "191ca079-b4fb-4a2a-870b-2853bcc34aeb": {"doc_hash": "93c4d07128710354dcd57429b829091c0173605e70616352144f85096440625d"}, "c31ddb12-4781-436d-bf48-95832d54aadc": {"doc_hash": "62c66717b5b75a377ba40bc69e169b21af7737d7893386698a8a28d150c78044"}, "a0eb4717-c10d-4331-a5e7-a4d0013fb213": {"doc_hash": "67f086fa9f2e92320d9822d43d4b6dace7265393097a16206972df22245a4c20"}, "73405ec1-cd73-4252-b91b-263a75586771": {"doc_hash": "68989d18e407b9e0f96c76700d9e1b58529de364f966e76c59faa8712374a369", "ref_doc_id": "c8f3d9f9-bd00-49af-9c33-34dbdb9fc164"}, "3dfbcb19-f2af-4afe-b261-f3ce78403d65": {"doc_hash": "de492d647b02ca059696fe1fe18f1db952d658ea316854f886fc6b4f7ffaf052", "ref_doc_id": "e6f11682-d4fb-447b-a354-8dc12d52df1c"}, "bf34d92a-80e1-4f77-83b8-dc2724509958": {"doc_hash": "06384e73f068e0629776438ed052ba49a13b4e041767f5ed1c95c72ad9bd49af", "ref_doc_id": "275c23d1-c723-4cf2-baf0-569475b091c6"}, "c602603b-c746-4ebe-96ea-1199f5e5a223": {"doc_hash": "34d51afaad1146903b4fd5253b017aca499287e87e8c18fc7939f27945020b5f", "ref_doc_id": "d9a8c255-934f-411e-9a1b-008a582f1d79"}, "33ee5900-cfaa-44d3-a67f-731d9ce1dfb9": {"doc_hash": "591622b3611642554639ceb53e2bd7659c0ca486c5e51066afebe37ad5c8b57b", "ref_doc_id": "b10cd7bc-9eee-45a7-9bfd-34dfc6eeaaef"}, "26b16ef0-fbcb-41d5-aa31-023a771d5d4f": {"doc_hash": "a02f8069a0b225e1e2c7e57c572c421a4abf0a582f4d9a5a97f026a1137d463d", "ref_doc_id": "9e7939ec-4b42-4798-9004-3c4e484a0736"}, "36110eba-c16b-4c3c-b6d0-08293de9367a": {"doc_hash": "d174551136c1716f7fd46ca883420a7b2c99a8a09d6aacb6c74827a160e97653", "ref_doc_id": "9da02523-aee4-474a-9216-df88f7e18304"}, "199750f9-a4ae-4c86-96ee-22a612f6e74b": {"doc_hash": "40f78dc4dc865224a94b9c1bd2a393e7ce8a86634a6c3333bfc906e1f04bd941", "ref_doc_id": "b2649ba2-dd10-48b4-9d40-ad82e540cb0d"}, "da826a22-c2cf-4cbb-8ff1-4c72c7108b81": {"doc_hash": "202c98f86e108e80b1930a1e7e0c0eb12f577cb85ca6e3df840a7fa88eb04a01", "ref_doc_id": "c5bbfdfb-e5c9-4f0c-a2ea-7cbf685e064a"}, "81e5acb5-1f64-4992-b472-81b721f81ca4": {"doc_hash": "1d0fad31cd5b63e7a23f00d9faf2ad6565e697383d9f9fdd13ee7f0a46e584f5", "ref_doc_id": "b0e71925-7911-4bf4-8add-e7cb4982922a"}, "bcc9daba-e205-4fc5-b816-5c59c7c8736d": {"doc_hash": "6da0bdc177870c06d2392b2dd45732fc8768d151cdd72503a429080161b7df9b", "ref_doc_id": "0afa5d2b-eb61-44be-bab9-cbf5fec37f75"}, "eed82e58-6567-4cb7-a058-9582ada3ed82": {"doc_hash": "c507ca281937f56052bed78f6b644f5c07799bcd5d9156ed780ddf5a50bb902a", "ref_doc_id": "8178b0ca-56f1-482d-a155-d738b208cbfd"}, "8e998039-97e6-4a9c-87a2-93c35ce98f78": {"doc_hash": "bd73f2e0d722383387ebf94c5e1d12de5ec8448a7569ba67fa1a5741301fea05", "ref_doc_id": "59b2c013-b14a-405a-bf6e-33b30869c76c"}, "4d0f4f0c-d56d-4407-b822-e0f93cc7629e": {"doc_hash": "ba725ee18bff2fae5e0e0138fcd5daca0faccff2873479a7a8965686c12945fe", "ref_doc_id": "34c90045-8f90-4fc1-8151-6a45be78a89d"}, "f3ead13c-cd2e-44a8-b581-dd2ced5449bb": {"doc_hash": "39ae53a966022729fe463596e2fb78106d32694bbd62bfc66f46c6718c05e9b5", "ref_doc_id": "ab8d5641-227d-49e5-876d-9eeebc047bb3"}, "24e96014-a210-4fc4-b307-0ae96434d7bc": {"doc_hash": "4f443703927796b4f5f425a195166c766c12470f80d2d5e3ede1332367b375d5", "ref_doc_id": "e600e4dc-be9e-4b52-8247-6536f10a76a1"}, "54a71e4e-ab59-4819-8fb7-faf9a6741b2b": {"doc_hash": "cc1a0cec1148febf757ead3b0c8a35ee0ef86a2928de3d80c3b704c693b1a99f", "ref_doc_id": "962bde0e-2bf5-4951-8671-25a5e5f98234"}, "504185a0-3250-4901-9a0c-2fc7cb74902a": {"doc_hash": "cc327c1f9681de3fff087c566a6536f30e10b58df81908e528e57bafbc008c23", "ref_doc_id": "198ffda9-909e-4e53-ac6a-656a2463e048"}, "c705d711-4a8d-46df-b3f0-aa43e299fb97": {"doc_hash": "a79d5a29fa0574a53e83631bffda7644c7d18a739eaadf14eb7951f549ddfb1a", "ref_doc_id": "a3598513-f8c2-42b1-bae8-60d457cd13ef"}, "a7d0c031-2cdb-4325-b9e7-7c1307428280": {"doc_hash": "06384e73f068e0629776438ed052ba49a13b4e041767f5ed1c95c72ad9bd49af", "ref_doc_id": "47febcaa-c535-403d-ae2c-a7e5818288f4"}, "6c32eac6-b411-4dba-9e62-77db1cffe855": {"doc_hash": "34d51afaad1146903b4fd5253b017aca499287e87e8c18fc7939f27945020b5f", "ref_doc_id": "30b14f95-3f04-4b25-882d-377f0a409946"}, "e566244e-d6a4-4298-b15f-17af72633e27": {"doc_hash": "591622b3611642554639ceb53e2bd7659c0ca486c5e51066afebe37ad5c8b57b", "ref_doc_id": "3d96794d-9715-4b05-af57-8f0bb9887cca"}, "39a36da2-a72f-4b77-92cf-5052f0d7d448": {"doc_hash": "a02f8069a0b225e1e2c7e57c572c421a4abf0a582f4d9a5a97f026a1137d463d", "ref_doc_id": "74263b98-7f95-4891-a063-a3d61dc21e4e"}, "2b72b131-29b8-4a3e-a0a3-5f281fbfb77c": {"doc_hash": "d174551136c1716f7fd46ca883420a7b2c99a8a09d6aacb6c74827a160e97653", "ref_doc_id": "0dea7eec-bf13-433e-bed3-f1d1d71670a6"}, "ddd14ef3-1a89-484c-b3e2-89d4d760a31a": {"doc_hash": "40f78dc4dc865224a94b9c1bd2a393e7ce8a86634a6c3333bfc906e1f04bd941", "ref_doc_id": "409a979f-7f4b-4c47-869d-c4c32f5ee14f"}, "0bd0f9e6-28df-4156-a8d0-9d132e11ddb0": {"doc_hash": "202c98f86e108e80b1930a1e7e0c0eb12f577cb85ca6e3df840a7fa88eb04a01", "ref_doc_id": "e6e0a46e-ac13-4ccd-bf95-c59e002bc570"}, "5e612114-61f4-47a6-ac59-409d9ed64a1f": {"doc_hash": "1d0fad31cd5b63e7a23f00d9faf2ad6565e697383d9f9fdd13ee7f0a46e584f5", "ref_doc_id": "6498b211-a87c-445d-9f6f-7d97469b1bb4"}, "5391525c-be6b-487f-89fe-d0ff7f4fe3d6": {"doc_hash": "6da0bdc177870c06d2392b2dd45732fc8768d151cdd72503a429080161b7df9b", "ref_doc_id": "5878b409-641a-459e-93df-f2b0da06c6d6"}, "14f956bd-0f64-4be6-a1f1-fbb327b04c50": {"doc_hash": "c507ca281937f56052bed78f6b644f5c07799bcd5d9156ed780ddf5a50bb902a", "ref_doc_id": "e62eccb5-333d-459c-90bc-dfa6f1789c1a"}, "d268625d-2c6a-4b29-8f78-1ccee89c7f26": {"doc_hash": "bd73f2e0d722383387ebf94c5e1d12de5ec8448a7569ba67fa1a5741301fea05", "ref_doc_id": "e62eccb5-333d-459c-90bc-dfa6f1789c1a"}, "3e9d5024-170f-40d2-8558-390e341ef828": {"doc_hash": "ba725ee18bff2fae5e0e0138fcd5daca0faccff2873479a7a8965686c12945fe", "ref_doc_id": "86d35ee7-a9e0-4e84-8f1d-03285f519f19"}, "b6397f60-6c77-4444-8b57-add3acc9dc55": {"doc_hash": "39ae53a966022729fe463596e2fb78106d32694bbd62bfc66f46c6718c05e9b5", "ref_doc_id": "a9959a8a-b143-47fd-8a92-8509de87b055"}, "69cee649-a768-40bd-8e3d-d0ff9ea296d4": {"doc_hash": "4f443703927796b4f5f425a195166c766c12470f80d2d5e3ede1332367b375d5", "ref_doc_id": "b3063718-9644-472b-8f59-d7dd1b67ef37"}, "7dca46f9-1f95-4c58-863b-e80638089423": {"doc_hash": "cc1a0cec1148febf757ead3b0c8a35ee0ef86a2928de3d80c3b704c693b1a99f", "ref_doc_id": "191ca079-b4fb-4a2a-870b-2853bcc34aeb"}, "e9338331-0bdb-4d20-bfd6-687f10ac360a": {"doc_hash": "cc327c1f9681de3fff087c566a6536f30e10b58df81908e528e57bafbc008c23", "ref_doc_id": "c31ddb12-4781-436d-bf48-95832d54aadc"}, "f1b7f119-65ba-4834-a82f-b882afc728aa": {"doc_hash": "a79d5a29fa0574a53e83631bffda7644c7d18a739eaadf14eb7951f549ddfb1a", "ref_doc_id": "a0eb4717-c10d-4331-a5e7-a4d0013fb213"}}, "docstore/ref_doc_info": {"c8f3d9f9-bd00-49af-9c33-34dbdb9fc164": {"node_ids": ["73405ec1-cd73-4252-b91b-263a75586771"], "metadata": {"id": "cybercrime_0", "title": "Fraud - Article 386 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "386", "lang": "en", "jurisdiction": "GR"}}, "e6f11682-d4fb-447b-a354-8dc12d52df1c": {"node_ids": ["3dfbcb19-f2af-4afe-b261-f3ce78403d65"], "metadata": {"id": "cybercrime_1", "title": "Unauthorized data access - Article 370\u0393 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "370", "lang": "en", "jurisdiction": "GR"}}, "275c23d1-c723-4cf2-baf0-569475b091c6": {"node_ids": ["bf34d92a-80e1-4f77-83b8-dc2724509958"], "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}}, "d9a8c255-934f-411e-9a1b-008a582f1d79": {"node_ids": ["c602603b-c746-4ebe-96ea-1199f5e5a223"], "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}}, "b10cd7bc-9eee-45a7-9bfd-34dfc6eeaaef": {"node_ids": ["33ee5900-cfaa-44d3-a67f-731d9ce1dfb9"], "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}}, "9e7939ec-4b42-4798-9004-3c4e484a0736": {"node_ids": ["26b16ef0-fbcb-41d5-aa31-023a771d5d4f"], "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}}, "9da02523-aee4-474a-9216-df88f7e18304": {"node_ids": ["36110eba-c16b-4c3c-b6d0-08293de9367a"], "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}}, "b2649ba2-dd10-48b4-9d40-ad82e540cb0d": {"node_ids": ["199750f9-a4ae-4c86-96ee-22a612f6e74b"], "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}}, "c5bbfdfb-e5c9-4f0c-a2ea-7cbf685e064a": {"node_ids": ["da826a22-c2cf-4cbb-8ff1-4c72c7108b81"], "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}}, "b0e71925-7911-4bf4-8add-e7cb4982922a": {"node_ids": ["81e5acb5-1f64-4992-b472-81b721f81ca4"], "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}}, "0afa5d2b-eb61-44be-bab9-cbf5fec37f75": {"node_ids": ["bcc9daba-e205-4fc5-b816-5c59c7c8736d"], "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}}, "8178b0ca-56f1-482d-a155-d738b208cbfd": {"node_ids": ["eed82e58-6567-4cb7-a058-9582ada3ed82"], "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}}, "59b2c013-b14a-405a-bf6e-33b30869c76c": {"node_ids": ["8e998039-97e6-4a9c-87a2-93c35ce98f78"], "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}}, "34c90045-8f90-4fc1-8151-6a45be78a89d": {"node_ids": ["4d0f4f0c-d56d-4407-b822-e0f93cc7629e"], "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}}, "ab8d5641-227d-49e5-876d-9eeebc047bb3": {"node_ids": ["f3ead13c-cd2e-44a8-b581-dd2ced5449bb"], "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}}, "e600e4dc-be9e-4b52-8247-6536f10a76a1": {"node_ids": ["24e96014-a210-4fc4-b307-0ae96434d7bc"], "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}}, "962bde0e-2bf5-4951-8671-25a5e5f98234": {"node_ids": ["54a71e4e-ab59-4819-8fb7-faf9a6741b2b"], "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}}, "198ffda9-909e-4e53-ac6a-656a2463e048": {"node_ids": ["504185a0-3250-4901-9a0c-2fc7cb74902a"], "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}}, "a3598513-f8c2-42b1-bae8-60d457cd13ef": {"node_ids": ["c705d711-4a8d-46df-b3f0-aa43e299fb97"], "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}}, "47febcaa-c535-403d-ae2c-a7e5818288f4": {"node_ids": ["a7d0c031-2cdb-4325-b9e7-7c1307428280"], "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}}, "30b14f95-3f04-4b25-882d-377f0a409946": {"node_ids": ["6c32eac6-b411-4dba-9e62-77db1cffe855"], "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}}, "3d96794d-9715-4b05-af57-8f0bb9887cca": {"node_ids": ["e566244e-d6a4-4298-b15f-17af72633e27"], "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}}, "74263b98-7f95-4891-a063-a3d61dc21e4e": {"node_ids": ["39a36da2-a72f-4b77-92cf-5052f0d7d448"], "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}}, "0dea7eec-bf13-433e-bed3-f1d1d71670a6": {"node_ids": ["2b72b131-29b8-4a3e-a0a3-5f281fbfb77c"], "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}}, "409a979f-7f4b-4c47-869d-c4c32f5ee14f": {"node_ids": ["ddd14ef3-1a89-484c-b3e2-89d4d760a31a"], "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}}, "e6e0a46e-ac13-4ccd-bf95-c59e002bc570": {"node_ids": ["0bd0f9e6-28df-4156-a8d0-9d132e11ddb0"], "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}}, "6498b211-a87c-445d-9f6f-7d97469b1bb4": {"node_ids": ["5e612114-61f4-47a6-ac59-409d9ed64a1f"], "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}}, "5878b409-641a-459e-93df-f2b0da06c6d6": {"node_ids": ["5391525c-be6b-487f-89fe-d0ff7f4fe3d6"], "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}}, "e62eccb5-333d-459c-90bc-dfa6f1789c1a": {"node_ids": ["14f956bd-0f64-4be6-a1f1-fbb327b04c50", "d268625d-2c6a-4b29-8f78-1ccee89c7f26"], "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}}, "86d35ee7-a9e0-4e84-8f1d-03285f519f19": {"node_ids": ["3e9d5024-170f-40d2-8558-390e341ef828"], "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}}, "a9959a8a-b143-47fd-8a92-8509de87b055": {"node_ids": ["b6397f60-6c77-4444-8b57-add3acc9dc55"], "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}}, "b3063718-9644-472b-8f59-d7dd1b67ef37": {"node_ids": ["69cee649-a768-40bd-8e3d-d0ff9ea296d4"], "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}}, "191ca079-b4fb-4a2a-870b-2853bcc34aeb": {"node_ids": ["7dca46f9-1f95-4c58-863b-e80638089423"], "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}}, "c31ddb12-4781-436d-bf48-95832d54aadc": {"node_ids": ["e9338331-0bdb-4d20-bfd6-687f10ac360a"], "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}}, "a0eb4717-c10d-4331-a5e7-a4d0013fb213": {"node_ids": ["f1b7f119-65ba-4834-a82f-b882afc728aa"], "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}}}, "docstore/data": {"73405ec1-cd73-4252-b91b-263a75586771": {"__data__": {"id_": "73405ec1-cd73-4252-b91b-263a75586771", "embedding": null, "metadata": {"id": "cybercrime_0", "title": "Fraud - Article 386 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "386", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8f3d9f9-bd00-49af-9c33-34dbdb9fc164", "node_type": "4", "metadata": {"id": "cybercrime_0", "title": "Fraud - Article 386 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "386", "lang": "en", "jurisdiction": "GR"}, "hash": "e722408823b732039f6e123598054d6921a1affb83127c3ddb9dce6d87e629c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Anyone who, by knowingly presenting false facts as true or by unlawfully concealing or withholding true facts, damages another person's property by persuading someone to act, omission, or tolerance with the aim of obtaining, for themselves or another, an unlawful financial gain from the damage to that property shall be punished with imprisonment, \"and if the damage caused is particularly great, with imprisonment of at least three (3) months and a fine.\" . If the damage caused exceeds a total of one hundred and twenty thousand (120,000) euros, imprisonment of up to ten (10) years and a fine shall be imposed. 2. If the fraud is directed directly against the legal entity of the Greek State, legal entities governed by public law, or local government organizations, and the damage caused exceeds a total of one hundred and twenty thousand (120,000) euros, a prison sentence of at least ten (10) years and a fine of up to one thousand (1,000) daily units shall be imposed. This offense shall be time-barred after twenty (20) years.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dfbcb19-f2af-4afe-b261-f3ce78403d65": {"__data__": {"id_": "3dfbcb19-f2af-4afe-b261-f3ce78403d65", "embedding": null, "metadata": {"id": "cybercrime_1", "title": "Unauthorized data access - Article 370\u0393 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "370", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6f11682-d4fb-447b-a354-8dc12d52df1c", "node_type": "4", "metadata": {"id": "cybercrime_1", "title": "Unauthorized data access - Article 370\u0393 \u03a0\u039a", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "370", "lang": "en", "jurisdiction": "GR"}, "hash": "2c4f4228fd3bebbb2000c81c66ce7292f78b8a65061507afdf11c5251f461e08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Everyone who obtains access to data recorded in a computer or in the external memory of a computer or transmitted by telecommunication systems shall be punished with imprisonment for up to six months or by a fine from 29 to 15,000 Euro, under the condition that these acts have been committed without right, especially in violation of prohibitions or of security measures taken by the legal holder. If the act concerns the international relations or the security of the State, he shall be punished according to Article 148. If the offender is in the service of the legal holder of the data, the act of the preceding paragraph shall be punished only if it has been explicitly prohibited by internal regulations or by a written decision of the holder or of a competent employee of his.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf34d92a-80e1-4f77-83b8-dc2724509958": {"__data__": {"id_": "bf34d92a-80e1-4f77-83b8-dc2724509958", "embedding": null, "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "275c23d1-c723-4cf2-baf0-569475b091c6", "node_type": "4", "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}, "hash": "a9a9ccfc90e8b4b86c762be4ca94e6353a8832c3cc535040011d50f6a9e36d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Artificial Intelligence-driven Framework and Legal\n\nAdvice Tools for Phishing Prevention and\n\nMitigation in Information Systems\n\n\n\nAILA - PROJECT ID: 15440 \n\nStart Date: 10/10/2023 - End Date: 09/10/2025. D2.3C: Anti-Phishing Adaptation Mechanisms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDOCUMENT INFORMATION\n\nDeliverable Number\n\nDue Date \n\nDeliverable Name\n\nD2.3C\n\n09.10.2025\n\nAnti-Phishing Adaptation Mechanisms\n\n\n\nEXECUTIVE SUMMARY\n\nThis document is deliverable D2.3C: Anti-Phishing Adaptation Mechanisms. The purpose of this report is to describe the methodology on how to produce recommendations using a reinforcement learning approach. The deliverable is part of Task 2.2: Adaptation Mechanisms: and is part of WP2: User Modeling, Adaptation and Recommendation Algorithms. Task 2.1: This task aims to ensure success of the adaptation process. Therefore, adaptation mechanisms will be studied in three different perspectives, namely, adaptivity of human factors, adaptivity of technology factors, and adaptivity of legal factors and user goals. Among others, certain adaptation mechanisms will be examined with regards to their efficiency and credibility. Initially conventional methods like rule-based, or content-based will be investigated. In the final prototype we will use domain adaptation methods such as mapping of the latent representations to the adapted ones (Gkillas et al. 2022). The specification of the recommendation mechanism using AI primarily focuses on the application of Large Language Models (LLMs) to structure domain knowledge related to phishing and cybersecurity. This includes integrating contextual requirements and legal considerations drawn from domain-specific documents. Through precise domain modeling, these models enable legal reasoning and form the backbone of intelligent, adaptive recommendation systems. In this deliverable, we present the methodology of Retrieval-Augmented Generation (RAG) and its role in powering the phishing recommendation mechanism. We detail the implementation process and examine alternative methodologies that can be considered when designing such systems. TABLE OF CONTENTS\n\nINTRODUCTION\t5\n\nRole of the Deliverable\t5\n\nRelationship to other AILA Deliverables\t5\n\nDocument Structure\t5\n\nSECTION A: Retrieval Augmented Generation\t5\n\nGeneral CONTEXT\t5\n\nData ingestion\t6\n\nData querying\t7\n\nLLMs and RAG\t8\n\nChallenges and Considerations\t9\n\nSECTION B: Developing a RAG \u2013 based chatbot in the legal domain on phishing\t10\n\nThe Llama Index\t10\n\nImplementation method\t10\n\nSECTION C: Alternative RAG approaches\t19\n\nGraphRAG\t19\n\nLightRAG\t20\n\nAgenticRAG\t20\n\nRAGFlow\t21\n\nCache-Augmented Generation (CAG)\t21\n\nCONCLUSIONS\t22\n\nREFERENCES\t23\n\n\n\n\n\n\n\n\n\nINTRODUCTION\n\nRole of the Deliverable\n\nPhishing is a type of cyber-attack that involves tricking individuals into disclosing personal, financial, or other sensitive information or unwittingly downloading malicious software. This is typically achieved through deceptive communications, often email messages that mimic legitimate organizations or individuals. These communications are designed to create a sense of urgency, fear, or curiosity in the victim, leading them to click on a link or attachment, or to provide sensitive information such as passwords, credit card details, or social security numbers. The purpose of this report is to describe a retrieval augmented generation (RAG) methodology on how to capture the legal domain using common sense knowledge and domain \u2013 specific documents on the legal framework related to the phishing attacks. Such documents may be laws, regulations, previous trials, scientific reports etc.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c602603b-c746-4ebe-96ea-1199f5e5a223": {"__data__": {"id_": "c602603b-c746-4ebe-96ea-1199f5e5a223", "embedding": null, "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9a8c255-934f-411e-9a1b-008a582f1d79", "node_type": "4", "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}, "hash": "0ec5e9f2418528a9a0c9881e05f3b2b5a68d29b52c0e2f0442e15b51a4fd4e28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To this end, in this deliverable we expand on Retrieval Augmented Generation structures that exist in the literature and explain how one can implement RAG in the AILA project to develop agents that provide information on phishing. Relationship to other AILA Deliverables\n\nDeliverable \n\n\n\nRelation\n\nD2.1: AI-driven User Modeling Mechanism. Models the phishing problem with emphasis on the users. D2.2: Classification & Recommendation Mechanism\n\nAnalyzes how to adapt a general-purpose LLM model to new data and new situations arising using supervised learning. Document Structure \n\nThe rest of the document is structured as follows: Section A gives an overview of RAG and related methods. Section B describes how we implement the methods of RAG in our case. Section C explores alternative RAG approaches that merit consideration for future enhancement. SECTION A: Retrieval Augmented Generation\n\nGeneral CONTEXT\n\nRetrieval Augmented Generation (RAG) is a technique that integrates large language models (LLMs) with external knowledge sources to produce contextually informed and more accurate outputs. Rather than relying solely on the model\u2019s internal parameters and memorized patterns, RAG explicitly queries a retrieval component\u2014such as a search index, database, or knowledge graph\u2014at generation time. The language model then incorporates the retrieved content into its reasoning and response construction. In a typical RAG pipeline, when a user issues a query, the system first extracts relevant passages or documents from an external corpus based on semantic similarity or keyword matching. These retrieved documents are then provided to the language model as additional context. Leveraging this dynamically supplied information, the model generates an answer that is more grounded in factual, up-to-date data. This approach helps mitigate hallucinations, improves factual correctness, and allows the model\u2019s knowledge to be easily updated without retraining its parameters. As a result, RAG systems maintain the fluency and flexibility of large language models while enhancing their reliability and domain specificity through real-time access to curated information. Figure 1: Retrieval-augmented generation combines LLMs with embedding models and vector databases (from: https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/). When a user asks the LLM a question, the model first sends the query to another component that encodes it as a set of numerical values\u2014often referred to as an embedding or a vector. These embeddings are then compared against entries in a machine-readable index of a knowledge base. If the system finds one or more close matches, it retrieves the corresponding information, translates it back into human-readable text, and returns it to the LLM. The LLM then integrates this retrieved text with its own response before presenting a final answer to the user, potentially including references to the sources it identified (see Figure 1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33ee5900-cfaa-44d3-a67f-731d9ce1dfb9": {"__data__": {"id_": "33ee5900-cfaa-44d3-a67f-731d9ce1dfb9", "embedding": null, "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b10cd7bc-9eee-45a7-9bfd-34dfc6eeaaef", "node_type": "4", "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}, "hash": "c68de3cd673b554dafd5a0c3ab8969d3e4046cea2637df9f756910fb78e6bde3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Behind the scenes, the embedding model continuously builds and refreshes machine-readable indexes\u2014often referred to as vector databases\u2014to incorporate newly available or updated knowledge bases as they appear. Data ingestion\n\nHere\u2019s how the process can be broken down into a few sub-steps, all of which are highly modular. They include:\n\n1. Load Data Sources into Text: Convert data from various sources into text that can be processed further. 2. Chunk Text: Split the text into smaller segments. Since language models have a limit on the amount of text they can handle at once, working with smaller chunks is essential. 3. Embed Text: Generate a numeric embedding for each text chunk. This allows us to identify which chunks are most relevant to a given query by comparing their embeddings for similarity. 4. Load Embeddings into a Vector Store: Store both embeddings and their associated documents in a vector database. This helps to efficiently locate the most relevant chunks based on their embeddings. Figure 2: The ingestion process (from: https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)\n\n\n\nData querying\n\nThis process is broken down into a few steps. Like before, these are highly modular, and each step mainly depends on prompts that can be changed as needed. The steps include:\n\nCombine Chat History with the New Question: Merge the previous conversation context with the latest question into a standalone query. This ensures seamless follow-up questions, enhancing the overall user experience. Retrieve Relevant Documents: Use the embeddings and vector store created during the ingestion phase to fetch documents most relevant to the standalone query. Generate a Response: Using the standalone question and the retrieved documents, leverage a language model to generate a comprehensive and relevant response. Figure 3: The querying process (from: https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)\n\nLLMs and RAG\n\nIn AILA we evaluate both methods of LLM fine-tuning and RAG.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26b16ef0-fbcb-41d5-aa31-023a771d5d4f": {"__data__": {"id_": "26b16ef0-fbcb-41d5-aa31-023a771d5d4f", "embedding": null, "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e7939ec-4b42-4798-9004-3c4e484a0736", "node_type": "4", "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}, "hash": "2d92a9e7f1ae0991276e03634372dcf5d1ea7bac71723990b09fbad498bfc2b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main difference between supervised fine-tuning an LLM from Hugging Face and a RAG system trained in this way lies in their objectives and methodologies:\n\n1. Supervised Fine-Tuning: This process involves taking a pre-trained language model and further training it on a labeled dataset to improve its performance on specific tasks. The goal is to adapt the model to perform better on tasks like classification, summarization, or question answering by learning from examples with known outputs. This approach focuses on enhancing the model's ability to generate accurate responses based on the patterns in the training data. 2. RAG System: Combines retrieval and generation. It uses a retriever to fetch relevant documents or information from a database and then uses a language model to generate responses based on this retrieved context. The RAG system is designed to improve the factual accuracy and relevance of the generated content by grounding it in external data sources. This approach is particularly useful for tasks requiring up-to-date or domain-specific information. Index structures and LLM structures serve different purposes and are fundamentally different in their design and functionality:\n\n1. Index Structures: In LlamaIndex, index structures are data structures designed to organize and store data efficiently for retrieval. They include types like VectorStoreIndex, SummaryIndex, and TreeIndex. These structures are optimized for quick lookups and retrieval of relevant data points or documents based on queries. They work by organizing data into nodes, embeddings, or hierarchical trees to facilitate efficient search and retrieval operations. 2. LLM Structures: Large Language Models (LLMs) are neural network architectures designed to understand and generate human language. They consist of layers of neurons that process input text to produce output text, mostly using transformer architectures. LLMs are trained on vast amounts of text data to learn language patterns and semantics, enabling them to perform tasks like text generation, translation, and summarization. While both structures are used in AI applications, index structures focus on data organization and retrieval, whereas LLM structures focus on language understanding and generation. In AILA it appears that using a Retrieval-Augmented Generation (RAG) approach with LlamaIndex is generally more suitable than relying solely on a Language Model (LLM).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36110eba-c16b-4c3c-b6d0-08293de9367a": {"__data__": {"id_": "36110eba-c16b-4c3c-b6d0-08293de9367a", "embedding": null, "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9da02523-aee4-474a-9216-df88f7e18304", "node_type": "4", "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}, "hash": "5db10ae54641c9ee700f9430f18945999872a1cec2b8a54bcc2fcdf6e1006b1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG combines the strengths of retrieval and generation, allowing the chatbot to access and utilize up-to-date and relevant legal documents, which enhances the accuracy and reliability of the advice provided. RAG helps reduce hallucinations by grounding the model on retrieved context, increasing factuality, and making it easier to update or remove outdated or biased information. This is particularly important in the legal domain, where accuracy and current information are crucial. However we investigate both structures to gain from their complementarity: e.g., generate input questions from RAG to train the LLM, or \u00a0a fine-tuned LLM can better interpret the nuances of the retrieved documents, leading to more accurate and relevant answers. At this stage of the project the evaluation is common for the LLM and the RAG approach (see related section in D3.2). Challenges and Considerations\n\nDespite these advances, the use of LLMs in the legal domain (as with other domains as sell) comes with challenges, including instability issues due to high reward values, or KL approximations that can lead to negative values. SECTION B: Developing a RAG \u2013 based chatbot in the legal domain on phishing\n\nThe Llama Index\n\nTo implement Retrieval-Augmented Generation (RAG) we use the LlamaIndex. LlamaIndex offers several appealing features that differentiate it from other Retrieval-Augmented Generation (RAG) technologies and make it a competitive choice:\n\nData Connectors and Indexes: LlamaIndex provides data connectors to ingest data from various sources and formats, such as APIs, PDFs, and SQL databases. It structures data into intermediate representations that are optimized for LLM consumption, enhancing retrieval efficiency and accuracy. Advanced Query and Retrieval Interfaces: LlamaIndex offers advanced interfaces for feeding LLM input prompts and retrieving context-augmented outputs. This includes support for complex query workflows and LLM prompting, which can be customized for specific applications. Integration Flexibility: It can be integrated with other frameworks like LangChain and ChatGPT, providing deep integrations and additional functionalities. This flexibility allows users to leverage LlamaIndex as a tool within broader agent frameworks. LlamaIndex is structured around the concept of an \"Index,\" which is a data structure designed to enable efficient querying by a large language model (LLM). The core components of LlamaIndex include:\n\nDocument and Node Objects: Documents are the primary data units, which are split into smaller chunks called Nodes. These Nodes are the basic elements stored in the index. Index Types: LlamaIndex supports various index types, each optimized for different use cases:\n\nVector Store Index: This is the most common type, where documents are split into Nodes, and each Node is converted into a vector embedding. This allows for semantic search by comparing the similarity of embeddings. Summary Index: Stores documents in a way that facilitates generating summaries. It returns all documents to the query engine for summarization tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "199750f9-a4ae-4c86-96ee-22a612f6e74b": {"__data__": {"id_": "199750f9-a4ae-4c86-96ee-22a612f6e74b", "embedding": null, "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2649ba2-dd10-48b4-9d40-ad82e540cb0d", "node_type": "4", "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}, "hash": "60e7eb8b3013bd83dfc657a7b0bb30ca0c7fdb2df68f2bf26599642d4c4fbb3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tree Index: Organizes Nodes in a hierarchical tree structure, which can be traversed to find relevant information. Keyword Table Index: Maps keywords to Nodes, enabling keyword-based retrieval. Storage Context: The index structure is supported by a storage context, which includes interfaces for document storage, index storage, and vector storage. This modular design allows for flexibility in choosing storage backends, such as in-memory or external databases like MongoDB or Pinecone. Implementation method\n\nRAG Setup:\n\nLoad Documents and Create Index: When working with documents containing heavy text, it's important to break them down into smaller, manageable chunks. Creating an index may not be efficient for searching context in response to user queries. Therefore, a specific chunking strategy should be considered for each document. For example, when dealing with documents that contain articles on specific legislation, an effective approach is to chunk the document according to individual articles. Each document or chunk can also include metadata to aid in effective filtering based on the context of the user query. If the resulting chunks are still too large, consider creating sub-chunks while retaining similar metadata from the original chunks. The chunking strategy can vary; chunks can be split by sentence, by specific word or token sizes, or by incorporating overlaps to minimize the loss of relevant information between the sub-chunks of the unified chunk. Experiment with different chunk sizes and overlap sizes to control how documents are split into nodes. Generally, smaller chunks can lead to more precise embeddings, while larger chunks may capture broader context. Once this chunking process is complete, you can generate the index. This index will be used to retrieve relevant information based on user queries. You can experiment with different index types (e.g., Vector Store Index, Tree Index) and vector databases (e.g. ChromaDB[2], FAISS[1]) depending on your specific use case. In this demonstration, we will use the Vector Store Index. For index creation, an embedding model is utilized to generate numeric embeddings for each text chunk. In this example, OpenAI's embeddings are used internally, though you can opt to use your own embedding model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da826a22-c2cf-4cbb-8ff1-4c72c7108b81": {"__data__": {"id_": "da826a22-c2cf-4cbb-8ff1-4c72c7108b81", "embedding": null, "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5bbfdfb-e5c9-4f0c-a2ea-7cbf685e064a", "node_type": "4", "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}, "hash": "b9f52ee859dd4aa8ec453b245c353cdf4b5d7b08b10b805307db8bd56c67ae3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ensure consistency by using the same model for both indexing and querying. Some embeddings that are suitable for that task are the following:\n\nOpen Australian Legal Embeddings [3]\n\nLEGAL-BERT [4]\n\n\n\nBelow we provide a snippet of code for the creation of a Vector Index from a document that has been chunked. Here for simplicity we use OpenAI\u2019s embedding model \u201ctext-embedding-3-large\u201d. from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n\n# Load documents\n\nphishing_documents = []\n\n\n\nfor chunk in phishing_chunks:\n\n\u00a0 \u00a0 metadata = {\n\n\u00a0 \u00a0 \u00a0 \u00a0 'id':chunk['id']\n\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 for key in chunk['metadata'].keys():\n\n\u00a0 \u00a0 \u00a0 \u00a0 metadata[key] = chunk['metadata'][key]\n\n\n\n\u00a0 \u00a0 text = chunk['content']\n\n\u00a0 \u00a0 phishing_documents.append(Document(text=text,metadata=metadata))\n\n\n\n# Create an index\n\n index _retriever_openai = load_vector_index(\n\n \u00a0 \u00a0 \"vector_indexes/phishing_index_documents_openai\",\n\n \u00a0 \u00a0 OpenAIEmbedding(model=\"text-embedding-3-large\") \n\n )\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\u00a0 \u00a0 documents=phishing_documents,\n\n\u00a0 \u00a0 embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\")\n\n)\n\n\n\n\n\nQuery Engine: The created index can be utilized in two ways: 1) as a retriever to fetch relevant documents from the index, and 2) as a query engine, which combines retrieval and answer generation into a single process. The query engine can take multiple parameters that control both the documents retrieved from the index based on the query and the output generated. Internally, the query engine uses OpenAI's API to respond to user queries. Below are some of the parameters described:\n\nSimilarity_top_k: Number of top-k similar nodes to retrieve from the from the Vector Store for answering a specific query. Filters: These filters are used in order to search through the metadata of the documents in order to narrow the search space. Rerank: Whether to use a reranker to reorder the top-k results. Response_mode: When the Llama index is used as query engine we can define how the retrieved documents are synthesized in order to create a final answer that can be passed to the user. Options provided are the following:\n\n\u201crefine\u201d: creates and refines an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per retrieved chunk\n\n\u201ccompact\u201d: This is similar to \u201crefine\u201d mode but compact as the mode says. This concatenates the retrieved chunks, resulting in less LLM calls. \u201ctree summarize\u201d: The retrieved chunks are queried through a summary template on the LLM model, resulting in multiple answers. These answers are recursively used as chunks in a tree_summarize LLM call and so on, until there\u2019s only one chunk left, and thus only one final answer. \u201csimple_summarize\u201d: This truncates the retrieved chunks into a single LLM prompt. Good for quick summarization purposes, but may lose detail due to truncation\n\n\u201cno_text\u201d: This only returns the retrieved chunks without sending them to the LLM. Node_Postprocessors: Optional postprocessors can be applied to the retrieved nodes before the response synthesis step. Postprocessors can be used to filter out nodes based on their similarity, specific keywords. Postprocessors can also be used to introduce further reranking strategies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81e5acb5-1f64-4992-b472-81b721f81ca4": {"__data__": {"id_": "81e5acb5-1f64-4992-b472-81b721f81ca4", "embedding": null, "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0e71925-7911-4bf4-8add-e7cb4982922a", "node_type": "4", "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}, "hash": "67ceee059b1be00e0ace3a315b612ee34281bc5e025f6c03cc9e4b2302fb7c82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Text_qa_template / refine_template: Here we can define prompt templates that can be used during response generation. query_engine = index.as_query_engine()\n\n\n\nImproving the Retrieval-Augmented Generation (RAG) Pipeline:\n\nWhen the number of documents or text chunks is relatively small, a Retrieval-Augmented Generation (RAG) model performs efficiently. However, as the corpus scales, retrieving truly relevant context becomes increasingly computationally intensive. This can lead to slower responses and inconsistencies, especially when answering complex or specific queries. To maintain accuracy and responsiveness, several enhancement strategies can be incorporated into the RAG workflow. Metadata Filtering: Metadata filtering improves retrieval precision by narrowing the search space using structured attributes associated with each document or chunk (e.g., topic, language, jurisdiction). This method reduces noise and computational overhead by ensuring that only contextually relevant documents are considered during retrieval \u2014 even before vector similarity is calculated. Dynamic metadata filters can be applied based on query classification, which can be performed using LLMs or domain-specific encoders such as Legal-BERT. The effectiveness of metadata filtering depends on the richness and granularity of the metadata assigned during document indexing. query = \"What are the legal consequences of phishing in Greece?\"\n\nmetadata_filter = {\"topic\": \"Greek Penal Code\"}\n\nresults = vectorstore.similarity_search(query, filter=metadata_filter)\n\n\n\nEmbedding Model Fine-tuning: Embedding models lie at the heart of dense retrieval in RAG systems. While pretrained models like bge, E5, and MiniLM provide strong zero-shot performance, they often lack domain-specific nuance. Fine-tuning these models on curated (query, positive/negative context) examples significantly improves semantic alignment, especially in specialized domains such as law, healthcare, or finance. Training strategies may include:\n\nEmbedding training in a supervised manner by providing queries with corresponding positive and/or negative contexts to better distinguish semantic relationships. Unsupervised domain adaptation using in-domain corpora\n\nInstruction tuning, where structured prompts are used during fine-tuning to enhance intent encoding\n\nReranker [5]: As previously mentioned, we can input a reranker model as a parameter in the query engine. This step occurs after the initial retrieval phase and reorders the documents based on a deeper semantic understanding. Typically implemented as a cross-encoder (e.g., BERT, RoBERTa, LegalBERT), the reranker receives all initially retrieved documents are paired with the query, and a relevance score is computed using full cross-attention. Based on these scores, the top-k retrieved documents are reranked. This process effectively reduces hallucinations, filters out tangentially related documents, and ensures that only the most relevant and context-aligned chunks are passed to the LLM for generation. from sentence_transformers import CrossEncoder\n\n\n\ncandidate_docs = vector_db.query(\n\n    vector=query_embedding,\n\n    top_k=20,\n\n)\n\n\n\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n\n\ndef rerank(query, docs, top_k=5):\n\n    pairs = [(query, doc[\"text\"]) for doc in docs]\n\n    scores = reranker.predict(pairs)\n\n    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n\n    return [doc for doc, _ in reranked[:top_k]]\n\n\n\ntop_docs = rerank(query, candidate_docs)\n\n\n\nReranker Fine-tuning: Off-the-shelf reranker models are trained on general datasets and may not have specific knowledge of domain-specific data. Fine-tuning reranker models using domain-specific relevance judgments or synthetic approximations can drastically improve the reranking of the originally retrieved documents.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcc9daba-e205-4fc5-b816-5c59c7c8736d": {"__data__": {"id_": "bcc9daba-e205-4fc5-b816-5c59c7c8736d", "embedding": null, "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0afa5d2b-eb61-44be-bab9-cbf5fec37f75", "node_type": "4", "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}, "hash": "e2d250d7b3f0fd3b3a5e2d64d3aa795812be9cf4f147d442ef76fe5419164f5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this process, the data should be formatted as pairs, consisting of a query and a list of relevant passages or triplets that include negative passages or irrelevant documents. Platforms like Cohere also provide reranker models and reranker fine-tuning APIs (e.g., rerank-english-v2.0), allowing for tighter control over domain adaptation. Cohere: Cohere [6] is a powerful tool that offers applications based on large language models (LLMs) with a focus on enterprise use cases. Their models are optimized for multi-turn dialogue, summarization, classification tasks, and retrieval-augmented generation (RAG). Cohere provides both hosted APIs and on-premise deployment options, making it an attractive choice for scalable and secure chatbot applications. The tool is designed to be easily integrated into any custom application. Developers can access its capabilities via RESTful APIs or SDKs, such as the Cohere Python SDK, making it simple to generate AI completions, embeddings, and text classifications. The models support instruction-following tasks and are optimized for grounding in external contexts, which is essential for domain-specific applications such as legal, healthcare, and text documentation. One of Cohere's key strengths is its support for RAG-native workflows, as it can be used for both document retrieval and text generation. For retrieval purposes, it provides high-quality embedding models that support English text as well as multilingual text, including Greek (embed-v4.0). These embeddings can be stored in a vector database and utilized for semantic searches, similar to Llama-Index. Additionally, Cohere offers reranker models, such as rerank-english-v2.0, which can reorder retrieved documents based on their relevance to a user query, thus improving the quality of the final answer provided by the model. Furthermore, Cohere implements Command R models that are RAG-native, meaning they are designed to generate answers based on the text chunks retrieved based on user queries. These features make Cohere a strong backend for LLM-based search and question-answering systems, where accurate retrieval and reasoning generation are crucial. Cohere can also be integrated into the Llama-Index structure. Within LlamaIndex, Cohere can serve as a language model backend for generation, an embedding model for document indexing and retrieval, or a reranker to refine search results. This modular integration makes it easy to build and optimize custom RAG pipelines using Cohere's tools. Additionally, Cohere also offers the capability for model fine-tuning, allowing you to teach a model a new task or leverage new data. Fine-tuning within Cohere can be applied to specific tasks, such as creating chat conversations to enable chatbot-like behavior, text classification, and the reranking of documents. This is particularly useful for Retrieval-Augmented Generation (RAG).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eed82e58-6567-4cb7-a058-9582ada3ed82": {"__data__": {"id_": "eed82e58-6567-4cb7-a058-9582ada3ed82", "embedding": null, "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8178b0ca-56f1-482d-a155-d738b208cbfd", "node_type": "4", "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "hash": "761e29a06720e29595cd89591b127ac9cfab8b68ab18fa69409961647b2f6828", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To fine-tune a model for RAG, you need to provide a dataset that includes queries alongside relevant passages. This dataset should contain a list of documents or passages that answer the queries, as well as \"hard negatives\"\u2014examples that seem relevant to the query but do not actually contain the correct answer. The dataset should be at least 256 queries long. During the fine-tuning process, Cohere provides various metrics to evaluate effectiveness, including Accuracy@1, Accuracy@3, MMR@10, and nDCG@10. While direct fine-tuning of Cohere's proprietary models is not generally available to all users due to limited access in the free edition, there is still considerable potential to utilize Cohere effectively to enhance the Llama-Index structure for RAG applications. This allows developers to customize the system's behavior to meet specific domains or user needs, making it particularly effective for applications like legal chatbots, where accuracy and interpretability are critical. For the creation a finetuned reranker via Cohere, the data should be structured in JSON Lines format. Here are examples of how each line in the .jsonl\u00a0file should look:\n\n\n\nWithout Hard Negatives:\n\n{\n\n\u00a0 \u00a0 \"query\": \"What are the different types of phishing attacks?\u201d,\n\n\u00a0 \u00a0 \"relevant_passages\": [\"The different types of phishing attacks mentioned in the context are Email Phishing/Smishing, Spear Phishing, Social Media Phishing, and Vishing (voice phishing).\u201d]\n\n}\n\n\n\nWith Hard Negatives:\n\n{\n\n\u00a0 \"query\": \"What is the capital of France?\",\n\n\u00a0 \"relevant_passages\": [\"The different types of phishing attacks mentioned in the context are Email Phishing/Smishing, Spear Phishing, Social Media Phishing, and Vishing (voice phishing).\"],\n\n\u00a0 \"hard_negatives\": [\"Malware (Malicious Software) is not considered a phishing attack.\"]\n\n}\n\n\n\nEach JSON object should contain a\u00a0query, a list of\u00a0relevant_passages\u00a0that answer the query, and optionally, a list of\u00a0hard_negatives\u00a0that do not answer the query. Including hard negatives can improve the model's ability to distinguish between relevant and irrelevant information. After that we can upload it to the Cohere website and the finetuning begins. To use it we can do the following:\n\n\n\nImport cohere\n\n\n\nCOHERE_API_KEY = \"\" \u00a0# Replace with your Cohere API key\n\nco = cohere.ClientV2(COHERE_API_KEY)\n\nft = co.finetuning.get_finetuned_model(\u2018model_id\u2019)\n\n\n\nresponse = co.rerank(\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 query=query,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 documents=documents_texts,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model=cohere_model.finetuned_model.id + \"-ft\",\n\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\n\n\nHybrid Retrieval: Hybrid retrieval combines the strengths of dense (vector-based) and sparse (keyword-based) retrieval methods to maximize both semantic relevance and exact keyword matching. This approach is particularly effective in domains such as law and cybersecurity, where important concepts may not be semantically similar but are lexically precise. For dense retrieval, embedding-based models like BGE or E5, or fine-tuned domain-specific models, can be used. For sparse retrieval, traditional keyword-based searches using techniques like TF-IDF, BM25, or inverted indexes can be employed. Fusion methods (e.g., reciprocal rank fusion, weighted scoring) ensure that results are not only topically aligned but also lexically accurate. from rank_bm25 import BM25Okapi\n\n\n\n### Dense Retrieval\n\n\n\ndense_results = dense_vector_db.query(\n\n    vector=query_embedding,\n\n    top_k=10\n\n)\n\n\n\n### Sparse Retrieval\n\n\n\ntokenized_corpus = [doc[\"text\"].split() for doc in all_documents]\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n\n\nsparse_scores = bm25.get_scores(query.split())\n\ntop_sparse_indices = sorted(range(len(sparse_scores)), key=lambda i: sparse_scores[i], reverse=True)[:10]\n\nsparse_results = [all_documents[i] for i in top_sparse_indices]\n\n\n\n### Merge Results\n\n\n\ndef hybrid_merge(dense, sparse):\n\n    seen = set()\n\n    merged = []\n\n    for doc in dense + sparse:\n\n        doc_id = doc.get(\"id\") or doc[\"text\"]  # Simple dedup\n\n        if doc_id not in seen:\n\n            merged.append(doc)\n\n            seen.add(doc_id)\n\n    return merged[:15]  # or any top-k\n\n\n\nhybrid_docs = hybrid_merge(dense_results, sparse_results)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e998039-97e6-4a9c-87a2-93c35ce98f78": {"__data__": {"id_": "8e998039-97e6-4a9c-87a2-93c35ce98f78", "embedding": null, "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59b2c013-b14a-405a-bf6e-33b30869c76c", "node_type": "4", "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "hash": "42fc86baeeab0d450bc3fd044e00d152542dfd1663231e60120c6505c3158a1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Sparse Retrieval\n\n\n\ntokenized_corpus = [doc[\"text\"].split() for doc in all_documents]\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n\n\nsparse_scores = bm25.get_scores(query.split())\n\ntop_sparse_indices = sorted(range(len(sparse_scores)), key=lambda i: sparse_scores[i], reverse=True)[:10]\n\nsparse_results = [all_documents[i] for i in top_sparse_indices]\n\n\n\n### Merge Results\n\n\n\ndef hybrid_merge(dense, sparse):\n\n    seen = set()\n\n    merged = []\n\n    for doc in dense + sparse:\n\n        doc_id = doc.get(\"id\") or doc[\"text\"]  # Simple dedup\n\n        if doc_id not in seen:\n\n            merged.append(doc)\n\n            seen.add(doc_id)\n\n    return merged[:15]  # or any top-k\n\n\n\nhybrid_docs = hybrid_merge(dense_results, sparse_results)\n\n\n\n\n\nMulti-Query Expansion: Multi-query expansion enhances recall by generating semantically diverse variants of the original user query. It is a technique that transforms a single user query into multiple semantically diverse sub-queries. Each variation reflects a slightly different intent, framing, or lexical formulation of the original prompt. These variations are processed in parallel to retrieve a broader and more relevant document set. This can be achieved with:\n\nLarge Language Models (LLMs) by prompting them to rephrase the user query, or \n\nBy using paraphrase models like T5 or Pegasus.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d0f4f0c-d56d-4407-b822-e0f93cc7629e": {"__data__": {"id_": "4d0f4f0c-d56d-4407-b822-e0f93cc7629e", "embedding": null, "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34c90045-8f90-4fc1-8151-6a45be78a89d", "node_type": "4", "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}, "hash": "a58f32f554b8b47a718f3231b9e2f6a584dfbdd2f37c63ca498fee4624ddbc82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each query variant is embedded and processed independently. Retrieved results are merged, deduplicated, and reranked before being passed to the LLM. This method can expand recall coverage, especially for under-specified or ambiguous queries, surface relevant content with varied lexical overlaps, and trigger metadata filters more effectively. from transformers import pipeline\n\n\n\n### Expansion of Query\n\n\n\ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n\n\n\ndef expand_query(query, num=3):\n\n    prompt = f\"Generate {num} rephrasings of the following legal question:\\n'{query}'\"\n\n    output = generator(prompt, max_new_tokens=64)[0]['generated_text']\n\n    return [q.strip() for q in output.split('\\n') if q.strip()]\n\n    \n\nexpanded_queries = expand_query(\"What are the penalties for data breaches under Greek law?\")\n\n\n\n### Retrieve for each expanded Query\n\n\n\nall_results = []\n\nfor eq in expanded_queries:\n\n    eq_emb = embedder.encode(eq)\n\n    results = vector_db.query(vector=eq_emb, top_k=5)\n\n    all_results.extend(results)\n\n\n\n### Merge and deduplicate documents\n\n\n\ndef dedupe(docs):\n\n    seen = set()\n\n    merged = []\n\n    for doc in docs:\n\n        key = doc.get(\"id\") or doc[\"text\"]\n\n        if key not in seen:\n\n            merged.append(doc)\n\n            seen.add(key)\n\n    return merged\n\n\n\nunique_docs = dedupe(all_results)\n\n\n\n\n\nFeedback Loop Reinforcement: To enhance the long-term performance of the model, a feedback loop mechanism can be integrated into the retrieval-augmented generation (RAG) framework. This component allows the system to learn from its interactions with users by collecting and utilizing explicit and implicit feedback signals, thus improving the retrieval and generation process over time. This dynamic process enables the system to adapt to domain shifts, user expectations, and previous retrieval errors, resulting in more accurate and contextually aligned responses. User feedback can include thumbs up/down ratings on generated answers, selection of preferred documents for specific queries, or user engagement patterns, such as continuing the dialogue or disengaging after a single response, which can indicate satisfaction with previous results. Additionally, legal experts may manually review answers and provide correctness labels. This feedback can be integrated into various components of the RAG system, from further fine-tuning the embedding model and retriever to the reranker and answer generation component. While improvements to the internal components of the RAG system cannot occur in real-time, user feedback can be used to enhance answer generation. Evaluate and Iterate:\n\nAn effective strategy is to evaluate the model's performance on a validation set after each integration step, ensuring that changes lead to measurable improvements. This iterative approach allows us to leverage the strengths of RAG-based retrieval while refining the system\u2019s performance for our specific data and legal use cases\n\n\n\nThe processes outlined above can be orchestrated within an agentic workflow using LangChain [7]- a powerful framework designed to construct intelligent, modular pipelines built around LLM-driven agents. LangChain enables each component of the RAG system to be handled by specialized agents, each with distinct roles and responsibilities. Unlike traditional static workflows, LangChain\u2019s agentic model introduces dynamic reasoning, self-reflection, and decision-making, allowing the system to adapt in real-time to ambiguous queries, multilingual inputs, and evolving contexts. This flexibility empowers the pipeline to respond with both precision and resilience, even in complex legal or multilingual domains. SECTION C: Alternative RAG approaches\n\nAs the field of Retrieval-Augmented Generation (RAG) continues to evolve, more advanced and modular pipeline architectures are being developed to address real-world scenarios in the legal, biomedical, enterprise, and multilingual domains. While frameworks like LangChain and Haystack[8] provide a solid foundation for basic RAG implementations, certain use cases require specialized architectures or systems that go beyond the scope of using vector indexes and similarity for document retrieval. GraphRAG\n\nGraphRAG [9] integrates structured knowledge representations \u2013 such as ontologies, entity-relation graphs, and RDF triples - into the retrieval pipeline. When analyzing documents, agents identify semantic relationships and attempt to construct a knowledge graph from the corpus. This transformation makes retrieval a semantic traversal task rather than relying solely on vector similarity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3ead13c-cd2e-44a8-b581-dd2ced5449bb": {"__data__": {"id_": "f3ead13c-cd2e-44a8-b581-dd2ced5449bb", "embedding": null, "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab8d5641-227d-49e5-876d-9eeebc047bb3", "node_type": "4", "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}, "hash": "eedd110535ecdbdf93befdcfc811fa55d7125006348aa48a258fbfc29bd9fe88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of depending on unstructured document chunks, the system queries a graph-based representation of knowledge to extract or infer relevant entities, facts, and logical constraints. This approach is particularly beneficial in regulated domains like law or healthcare, where factual consistency and explainability are crucial. However, GraphRAG faces challenges in terms of maintainability and scalability; adding new documents often requires partial or complete regeneration of the graph structure, which can become a bottleneck in rapidly changing corpora or streaming data environments. Techniques such as incremental graph construction and knowledge graph pruning can be explored to address these limitations. Figure 4 Knowledge Graph Pipeline for creating the graph and its use when user inputs a query [10]. LightRAG\n\nLightRAG [11] is a streamlined variant of the RAG pipeline designed for environments where computational efficiency is critical, such as mobile deployments, edge computing, or real-time applications. It eliminates optional components, like rerankers, metadata filters, or multi-index routing, to simplify retrieval and response generation. While this might reduce answer precision in certain domains, it results in faster inference, lower power consumption, and significantly reduced infrastructure costs. LightRAG is especially advantageous in high-throughput settings, including customer support bots, embedded legal advisors, and multilingual FAQ systems\u2014where response latency must be tightly controlled. AgenticRAG\n\nAgentic RAG [12] introduces autonomous agents into the RAG workflow, allowing LLM-powered agents to make decisions about which tools to invoke, including retrieval, reasoning, summarization, and external APIs. In this model, retrieval is not a fixed step; it is one of many tools available in an agent\u2019s toolbox. Agents dynamically determine whether retrieval is necessary, how to phrase the query, whether to reformulate it, and how to combine results from multiple sources. This architecture promotes adaptive decision-making, complex multi-step workflows, and intelligent fallback strategies, making it particularly powerful in legal research, case triage, and compliance pipelines. When combined with agent orchestration frameworks like LangGraph or CrewAI, Agentic RAG evolves into an intelligent knowledge worker. RAGFlow\n\nRAGFlow [13] emphasizes the data preparation layer, which is often overlooked but essential for effective RAG performance. It provides robust preprocessing, chunking, embedding, and routing mechanisms designed for high-volume enterprise data. RAGFlow can process heterogeneous inputs, such as scanned contracts, spreadsheets, PDFs, and multilingual documents, into structured, retrievable vector formats. It supports conditional chunking strategies\u2014like title-aware, table-sensitive, and legal clause-based chunking\u2014and pipeline parallelization to scale across hundreds of thousands of documents. Its modularity makes it well-suited for long document RAG, contract analysis, medical record processing, and enterprise search. When paired with hybrid retrieval and re-ranking, RAGFlow ensures both high recall and semantic precision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24e96014-a210-4fc4-b307-0ae96434d7bc": {"__data__": {"id_": "24e96014-a210-4fc4-b307-0ae96434d7bc", "embedding": null, "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e600e4dc-be9e-4b52-8247-6536f10a76a1", "node_type": "4", "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}, "hash": "27cf414f7c73ab11cfb763850ea2b19ba35f7d0a327ffc049da2bc40b6dff2bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cache-Augmented Generation (CAG)\n\nIn a classic RAG pipeline, the system first pulls top-ranked documents or text chunks, inputs these retrieved chunks into the user\u2019s query and a Large Language Model processes the augmented prompt to produce the final output. However, each query triggers document retrieval, which can slow down responses and there is a possibility of selecting irrelevant or outdated documents which means that the output suffers. In contrast, Cache-Augmented Generation (CAG) [14] reverses this paradigm. In CAG, relevant context is preloaded into the extended context window of a large model (e.g., via a sliding cache or prompt streaming), and its attention weights or KV cache are retained throughout a session. This approach eliminates the need for runtime retrieval, enabling zero-latency, low-drift generation with cached semantic context. CAG is particularly beneficial for high-interaction sessions (e.g., legal advisory bots or code assistants) where topics evolve slowly over time, allowing cached documents to be utilized for multiple queries without degrading performance. To avoid context overflow, CAG systems often implement smart cache invalidation and chunk management strategies. However, there are some trade-offs, with the most important being the high cost of tokens. Since the entire cached context\u2014often tens of thousands of tokens\u2014is processed with each generation, the costs for input tokens can become exorbitant, especially with commercially priced language models like GPT-4 or Claude. This makes CAG potentially cost-prohibitive on a large scale, unless it is paired with aggressive summarization, token pruning, or semantic compression techniques. Another limitation of CAG is context staleness. As the cached knowledge remains static during a session, any updates, corrections, or deletions made to the source documents won't be reflected unless the cache is explicitly invalidated and refreshed. This process reintroduces the retrieval step and its associated latency, which can be problematic in fields where document updates are frequent or critical for legal or factual accuracy. Moreover, even with the introduction of 128K or 1M-token context windows, there are practical limits on how much content can be cached, particularly when considering memory or performance constraints. To maintain efficiency and relevance, intelligent cache management strategies\u2014such as semantic deduplication, importance-weighted chunk prioritization, and adaptive window shifting\u2014are often necessary. CONCLUSIONS\n\nIn this deliverable, we provided a comprehensive overview of how Retrieval-Augmented Generation (RAG) works. We detailed our implementation of RAG in the AILA framework to develop agents that effectively provide information on phishing. Specifically, we described a RAG pipeline that encompasses data ingestion, querying, and document retrieval. Additionally, we discussed several enhancements that can be made to improve RAG performance, such as introducing rerankers, applying metadata filtering, and fine-tuning embeddings. Lastly, we shared information about alternative strategies that resemble RAG and can be used as substitutes for a straightforward RAG pipeline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54a71e4e-ab59-4819-8fb7-faf9a6741b2b": {"__data__": {"id_": "54a71e4e-ab59-4819-8fb7-faf9a6741b2b", "embedding": null, "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "962bde0e-2bf5-4951-8671-25a5e5f98234", "node_type": "4", "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}, "hash": "93c4d07128710354dcd57429b829091c0173605e70616352144f85096440625d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "REFERENCES\u00a0 \n\n[1] Douze, Matthijs, Alexandr Guzhva, Chengqi Deng, et al. \u201cThe Faiss Library.\u201d Version 3. Preprint, arXiv, 2024. https://doi.org/10.48550/ARXIV.2401.08281. [2] https://docs.trychroma.com/docs/overview/introduction\n\n[3] https://huggingface.co/datasets/umarbutler/open-australian-legal-embeddings\n\n[4] https://huggingface.co/nlpaueb/legal-bert-base-uncased\n\n[5] X. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero\u2011Shot Listwise Document Reranking with a Large Language Model,\u201d CoRR, vol. abs/2305.02156, May\u202f2023. doi:\u202f10.48550/arXiv.2305.02156\n\n[6] https://cohere.com/\n\n[7] Topsakal, O., & Akinci, T. C. (2023, July). Creating large language model applications utilizing langchain: A primer on developing llm apps fast. In\u00a0International conference on applied engineering and natural sciences\u00a0(Vol. 1, No. 1, pp. 1050-1056). [8] https://haystack.deepset.ai\n\n[9] Han, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y., ... & Tang, J.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "504185a0-3250-4901-9a0c-2fc7cb74902a": {"__data__": {"id_": "504185a0-3250-4901-9a0c-2fc7cb74902a", "embedding": null, "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "198ffda9-909e-4e53-ac6a-656a2463e048", "node_type": "4", "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}, "hash": "62c66717b5b75a377ba40bc69e169b21af7737d7893386698a8a28d150c78044", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2024). Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309. [10] https://www.nebula-graph.io/posts/graph-RAG\n\n[11] Guo, Z., Xia, L., Yu, Y., Ao, T., & Huang, C. (2024). Lightrag: Simple and fast retrieval-augmented generation. arXiv preprint arXiv:2410.05779. [12] Singh, A., Ehtesham, A., Kumar, S., & Khoei, T. T. (2025). Agentic retrieval-augmented generation: A survey on agentic rag. arXiv preprint arXiv:2501.09136. [13] https://github.com/infiniflow/ragflow\n\n[14] Chan, B. J., Chen, C. T., Cheng, J. H., & Huang, H. H. (2025, May). Don't do rag: When cache-augmented generation is all you need for knowledge tasks. In\u00a0Companion Proceedings of the ACM on Web Conference 2025\u00a0(pp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c705d711-4a8d-46df-b3f0-aa43e299fb97": {"__data__": {"id_": "c705d711-4a8d-46df-b3f0-aa43e299fb97", "embedding": null, "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3598513-f8c2-42b1-bae8-60d457cd13ef", "node_type": "4", "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}, "hash": "c376b0f042cc89ce677b4dd016c2b6ca3722f6c67d8f3cfee21f543546190159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "893-897).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7d0c031-2cdb-4325-b9e7-7c1307428280": {"__data__": {"id_": "a7d0c031-2cdb-4325-b9e7-7c1307428280", "embedding": null, "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47febcaa-c535-403d-ae2c-a7e5818288f4", "node_type": "4", "metadata": {"id": "cybercrime_0", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "0", "lang": "en", "jurisdiction": "GR"}, "hash": "f3507e7c8285e100619cbd473379bd6fcbe3866b7fa2f8bdb547530eb6ffbdaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Artificial Intelligence-driven Framework and Legal\n\nAdvice Tools for Phishing Prevention and\n\nMitigation in Information Systems\n\n\n\nAILA - PROJECT ID: 15440 \n\nStart Date: 10/10/2023 - End Date: 09/10/2025. D2.3C: Anti-Phishing Adaptation Mechanisms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDOCUMENT INFORMATION\n\nDeliverable Number\n\nDue Date \n\nDeliverable Name\n\nD2.3C\n\n09.10.2025\n\nAnti-Phishing Adaptation Mechanisms\n\n\n\nEXECUTIVE SUMMARY\n\nThis document is deliverable D2.3C: Anti-Phishing Adaptation Mechanisms. The purpose of this report is to describe the methodology on how to produce recommendations using a reinforcement learning approach. The deliverable is part of Task 2.2: Adaptation Mechanisms: and is part of WP2: User Modeling, Adaptation and Recommendation Algorithms. Task 2.1: This task aims to ensure success of the adaptation process. Therefore, adaptation mechanisms will be studied in three different perspectives, namely, adaptivity of human factors, adaptivity of technology factors, and adaptivity of legal factors and user goals. Among others, certain adaptation mechanisms will be examined with regards to their efficiency and credibility. Initially conventional methods like rule-based, or content-based will be investigated. In the final prototype we will use domain adaptation methods such as mapping of the latent representations to the adapted ones (Gkillas et al. 2022). The specification of the recommendation mechanism using AI primarily focuses on the application of Large Language Models (LLMs) to structure domain knowledge related to phishing and cybersecurity. This includes integrating contextual requirements and legal considerations drawn from domain-specific documents. Through precise domain modeling, these models enable legal reasoning and form the backbone of intelligent, adaptive recommendation systems. In this deliverable, we present the methodology of Retrieval-Augmented Generation (RAG) and its role in powering the phishing recommendation mechanism. We detail the implementation process and examine alternative methodologies that can be considered when designing such systems. TABLE OF CONTENTS\n\nINTRODUCTION\t5\n\nRole of the Deliverable\t5\n\nRelationship to other AILA Deliverables\t5\n\nDocument Structure\t5\n\nSECTION A: Retrieval Augmented Generation\t5\n\nGeneral CONTEXT\t5\n\nData ingestion\t6\n\nData querying\t7\n\nLLMs and RAG\t8\n\nChallenges and Considerations\t9\n\nSECTION B: Developing a RAG \u2013 based chatbot in the legal domain on phishing\t10\n\nThe Llama Index\t10\n\nImplementation method\t10\n\nSECTION C: Alternative RAG approaches\t19\n\nGraphRAG\t19\n\nLightRAG\t20\n\nAgenticRAG\t20\n\nRAGFlow\t21\n\nCache-Augmented Generation (CAG)\t21\n\nCONCLUSIONS\t22\n\nREFERENCES\t23\n\n\n\n\n\n\n\n\n\nINTRODUCTION\n\nRole of the Deliverable\n\nPhishing is a type of cyber-attack that involves tricking individuals into disclosing personal, financial, or other sensitive information or unwittingly downloading malicious software. This is typically achieved through deceptive communications, often email messages that mimic legitimate organizations or individuals. These communications are designed to create a sense of urgency, fear, or curiosity in the victim, leading them to click on a link or attachment, or to provide sensitive information such as passwords, credit card details, or social security numbers. The purpose of this report is to describe a retrieval augmented generation (RAG) methodology on how to capture the legal domain using common sense knowledge and domain \u2013 specific documents on the legal framework related to the phishing attacks. Such documents may be laws, regulations, previous trials, scientific reports etc.", "mimetype": "text/plain", "start_char_idx": 24, "end_char_idx": 3633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c32eac6-b411-4dba-9e62-77db1cffe855": {"__data__": {"id_": "6c32eac6-b411-4dba-9e62-77db1cffe855", "embedding": null, "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30b14f95-3f04-4b25-882d-377f0a409946", "node_type": "4", "metadata": {"id": "cybercrime_1", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "1", "lang": "en", "jurisdiction": "GR"}, "hash": "0ec5e9f2418528a9a0c9881e05f3b2b5a68d29b52c0e2f0442e15b51a4fd4e28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To this end, in this deliverable we expand on Retrieval Augmented Generation structures that exist in the literature and explain how one can implement RAG in the AILA project to develop agents that provide information on phishing. Relationship to other AILA Deliverables\n\nDeliverable \n\n\n\nRelation\n\nD2.1: AI-driven User Modeling Mechanism. Models the phishing problem with emphasis on the users. D2.2: Classification & Recommendation Mechanism\n\nAnalyzes how to adapt a general-purpose LLM model to new data and new situations arising using supervised learning. Document Structure \n\nThe rest of the document is structured as follows: Section A gives an overview of RAG and related methods. Section B describes how we implement the methods of RAG in our case. Section C explores alternative RAG approaches that merit consideration for future enhancement. SECTION A: Retrieval Augmented Generation\n\nGeneral CONTEXT\n\nRetrieval Augmented Generation (RAG) is a technique that integrates large language models (LLMs) with external knowledge sources to produce contextually informed and more accurate outputs. Rather than relying solely on the model\u2019s internal parameters and memorized patterns, RAG explicitly queries a retrieval component\u2014such as a search index, database, or knowledge graph\u2014at generation time. The language model then incorporates the retrieved content into its reasoning and response construction. In a typical RAG pipeline, when a user issues a query, the system first extracts relevant passages or documents from an external corpus based on semantic similarity or keyword matching. These retrieved documents are then provided to the language model as additional context. Leveraging this dynamically supplied information, the model generates an answer that is more grounded in factual, up-to-date data. This approach helps mitigate hallucinations, improves factual correctness, and allows the model\u2019s knowledge to be easily updated without retraining its parameters. As a result, RAG systems maintain the fluency and flexibility of large language models while enhancing their reliability and domain specificity through real-time access to curated information. Figure 1: Retrieval-augmented generation combines LLMs with embedding models and vector databases (from: https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/). When a user asks the LLM a question, the model first sends the query to another component that encodes it as a set of numerical values\u2014often referred to as an embedding or a vector. These embeddings are then compared against entries in a machine-readable index of a knowledge base. If the system finds one or more close matches, it retrieves the corresponding information, translates it back into human-readable text, and returns it to the LLM. The LLM then integrates this retrieved text with its own response before presenting a final answer to the user, potentially including references to the sources it identified (see Figure 1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e566244e-d6a4-4298-b15f-17af72633e27": {"__data__": {"id_": "e566244e-d6a4-4298-b15f-17af72633e27", "embedding": null, "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d96794d-9715-4b05-af57-8f0bb9887cca", "node_type": "4", "metadata": {"id": "cybercrime_2", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "2", "lang": "en", "jurisdiction": "GR"}, "hash": "c68de3cd673b554dafd5a0c3ab8969d3e4046cea2637df9f756910fb78e6bde3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Behind the scenes, the embedding model continuously builds and refreshes machine-readable indexes\u2014often referred to as vector databases\u2014to incorporate newly available or updated knowledge bases as they appear. Data ingestion\n\nHere\u2019s how the process can be broken down into a few sub-steps, all of which are highly modular. They include:\n\n1. Load Data Sources into Text: Convert data from various sources into text that can be processed further. 2. Chunk Text: Split the text into smaller segments. Since language models have a limit on the amount of text they can handle at once, working with smaller chunks is essential. 3. Embed Text: Generate a numeric embedding for each text chunk. This allows us to identify which chunks are most relevant to a given query by comparing their embeddings for similarity. 4. Load Embeddings into a Vector Store: Store both embeddings and their associated documents in a vector database. This helps to efficiently locate the most relevant chunks based on their embeddings. Figure 2: The ingestion process (from: https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)\n\n\n\nData querying\n\nThis process is broken down into a few steps. Like before, these are highly modular, and each step mainly depends on prompts that can be changed as needed. The steps include:\n\nCombine Chat History with the New Question: Merge the previous conversation context with the latest question into a standalone query. This ensures seamless follow-up questions, enhancing the overall user experience. Retrieve Relevant Documents: Use the embeddings and vector store created during the ingestion phase to fetch documents most relevant to the standalone query. Generate a Response: Using the standalone question and the retrieved documents, leverage a language model to generate a comprehensive and relevant response. Figure 3: The querying process (from: https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)\n\nLLMs and RAG\n\nIn AILA we evaluate both methods of LLM fine-tuning and RAG.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39a36da2-a72f-4b77-92cf-5052f0d7d448": {"__data__": {"id_": "39a36da2-a72f-4b77-92cf-5052f0d7d448", "embedding": null, "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74263b98-7f95-4891-a063-a3d61dc21e4e", "node_type": "4", "metadata": {"id": "cybercrime_3", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "3", "lang": "en", "jurisdiction": "GR"}, "hash": "2d92a9e7f1ae0991276e03634372dcf5d1ea7bac71723990b09fbad498bfc2b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main difference between supervised fine-tuning an LLM from Hugging Face and a RAG system trained in this way lies in their objectives and methodologies:\n\n1. Supervised Fine-Tuning: This process involves taking a pre-trained language model and further training it on a labeled dataset to improve its performance on specific tasks. The goal is to adapt the model to perform better on tasks like classification, summarization, or question answering by learning from examples with known outputs. This approach focuses on enhancing the model's ability to generate accurate responses based on the patterns in the training data. 2. RAG System: Combines retrieval and generation. It uses a retriever to fetch relevant documents or information from a database and then uses a language model to generate responses based on this retrieved context. The RAG system is designed to improve the factual accuracy and relevance of the generated content by grounding it in external data sources. This approach is particularly useful for tasks requiring up-to-date or domain-specific information. Index structures and LLM structures serve different purposes and are fundamentally different in their design and functionality:\n\n1. Index Structures: In LlamaIndex, index structures are data structures designed to organize and store data efficiently for retrieval. They include types like VectorStoreIndex, SummaryIndex, and TreeIndex. These structures are optimized for quick lookups and retrieval of relevant data points or documents based on queries. They work by organizing data into nodes, embeddings, or hierarchical trees to facilitate efficient search and retrieval operations. 2. LLM Structures: Large Language Models (LLMs) are neural network architectures designed to understand and generate human language. They consist of layers of neurons that process input text to produce output text, mostly using transformer architectures. LLMs are trained on vast amounts of text data to learn language patterns and semantics, enabling them to perform tasks like text generation, translation, and summarization. While both structures are used in AI applications, index structures focus on data organization and retrieval, whereas LLM structures focus on language understanding and generation. In AILA it appears that using a Retrieval-Augmented Generation (RAG) approach with LlamaIndex is generally more suitable than relying solely on a Language Model (LLM).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b72b131-29b8-4a3e-a0a3-5f281fbfb77c": {"__data__": {"id_": "2b72b131-29b8-4a3e-a0a3-5f281fbfb77c", "embedding": null, "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dea7eec-bf13-433e-bed3-f1d1d71670a6", "node_type": "4", "metadata": {"id": "cybercrime_4", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "4", "lang": "en", "jurisdiction": "GR"}, "hash": "5db10ae54641c9ee700f9430f18945999872a1cec2b8a54bcc2fcdf6e1006b1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG combines the strengths of retrieval and generation, allowing the chatbot to access and utilize up-to-date and relevant legal documents, which enhances the accuracy and reliability of the advice provided. RAG helps reduce hallucinations by grounding the model on retrieved context, increasing factuality, and making it easier to update or remove outdated or biased information. This is particularly important in the legal domain, where accuracy and current information are crucial. However we investigate both structures to gain from their complementarity: e.g., generate input questions from RAG to train the LLM, or \u00a0a fine-tuned LLM can better interpret the nuances of the retrieved documents, leading to more accurate and relevant answers. At this stage of the project the evaluation is common for the LLM and the RAG approach (see related section in D3.2). Challenges and Considerations\n\nDespite these advances, the use of LLMs in the legal domain (as with other domains as sell) comes with challenges, including instability issues due to high reward values, or KL approximations that can lead to negative values. SECTION B: Developing a RAG \u2013 based chatbot in the legal domain on phishing\n\nThe Llama Index\n\nTo implement Retrieval-Augmented Generation (RAG) we use the LlamaIndex. LlamaIndex offers several appealing features that differentiate it from other Retrieval-Augmented Generation (RAG) technologies and make it a competitive choice:\n\nData Connectors and Indexes: LlamaIndex provides data connectors to ingest data from various sources and formats, such as APIs, PDFs, and SQL databases. It structures data into intermediate representations that are optimized for LLM consumption, enhancing retrieval efficiency and accuracy. Advanced Query and Retrieval Interfaces: LlamaIndex offers advanced interfaces for feeding LLM input prompts and retrieving context-augmented outputs. This includes support for complex query workflows and LLM prompting, which can be customized for specific applications. Integration Flexibility: It can be integrated with other frameworks like LangChain and ChatGPT, providing deep integrations and additional functionalities. This flexibility allows users to leverage LlamaIndex as a tool within broader agent frameworks. LlamaIndex is structured around the concept of an \"Index,\" which is a data structure designed to enable efficient querying by a large language model (LLM). The core components of LlamaIndex include:\n\nDocument and Node Objects: Documents are the primary data units, which are split into smaller chunks called Nodes. These Nodes are the basic elements stored in the index. Index Types: LlamaIndex supports various index types, each optimized for different use cases:\n\nVector Store Index: This is the most common type, where documents are split into Nodes, and each Node is converted into a vector embedding. This allows for semantic search by comparing the similarity of embeddings. Summary Index: Stores documents in a way that facilitates generating summaries. It returns all documents to the query engine for summarization tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddd14ef3-1a89-484c-b3e2-89d4d760a31a": {"__data__": {"id_": "ddd14ef3-1a89-484c-b3e2-89d4d760a31a", "embedding": null, "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "409a979f-7f4b-4c47-869d-c4c32f5ee14f", "node_type": "4", "metadata": {"id": "cybercrime_5", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "5", "lang": "en", "jurisdiction": "GR"}, "hash": "60e7eb8b3013bd83dfc657a7b0bb30ca0c7fdb2df68f2bf26599642d4c4fbb3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tree Index: Organizes Nodes in a hierarchical tree structure, which can be traversed to find relevant information. Keyword Table Index: Maps keywords to Nodes, enabling keyword-based retrieval. Storage Context: The index structure is supported by a storage context, which includes interfaces for document storage, index storage, and vector storage. This modular design allows for flexibility in choosing storage backends, such as in-memory or external databases like MongoDB or Pinecone. Implementation method\n\nRAG Setup:\n\nLoad Documents and Create Index: When working with documents containing heavy text, it's important to break them down into smaller, manageable chunks. Creating an index may not be efficient for searching context in response to user queries. Therefore, a specific chunking strategy should be considered for each document. For example, when dealing with documents that contain articles on specific legislation, an effective approach is to chunk the document according to individual articles. Each document or chunk can also include metadata to aid in effective filtering based on the context of the user query. If the resulting chunks are still too large, consider creating sub-chunks while retaining similar metadata from the original chunks. The chunking strategy can vary; chunks can be split by sentence, by specific word or token sizes, or by incorporating overlaps to minimize the loss of relevant information between the sub-chunks of the unified chunk. Experiment with different chunk sizes and overlap sizes to control how documents are split into nodes. Generally, smaller chunks can lead to more precise embeddings, while larger chunks may capture broader context. Once this chunking process is complete, you can generate the index. This index will be used to retrieve relevant information based on user queries. You can experiment with different index types (e.g., Vector Store Index, Tree Index) and vector databases (e.g. ChromaDB[2], FAISS[1]) depending on your specific use case. In this demonstration, we will use the Vector Store Index. For index creation, an embedding model is utilized to generate numeric embeddings for each text chunk. In this example, OpenAI's embeddings are used internally, though you can opt to use your own embedding model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bd0f9e6-28df-4156-a8d0-9d132e11ddb0": {"__data__": {"id_": "0bd0f9e6-28df-4156-a8d0-9d132e11ddb0", "embedding": null, "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6e0a46e-ac13-4ccd-bf95-c59e002bc570", "node_type": "4", "metadata": {"id": "cybercrime_6", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "6", "lang": "en", "jurisdiction": "GR"}, "hash": "b9f52ee859dd4aa8ec453b245c353cdf4b5d7b08b10b805307db8bd56c67ae3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ensure consistency by using the same model for both indexing and querying. Some embeddings that are suitable for that task are the following:\n\nOpen Australian Legal Embeddings [3]\n\nLEGAL-BERT [4]\n\n\n\nBelow we provide a snippet of code for the creation of a Vector Index from a document that has been chunked. Here for simplicity we use OpenAI\u2019s embedding model \u201ctext-embedding-3-large\u201d. from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n\n# Load documents\n\nphishing_documents = []\n\n\n\nfor chunk in phishing_chunks:\n\n\u00a0 \u00a0 metadata = {\n\n\u00a0 \u00a0 \u00a0 \u00a0 'id':chunk['id']\n\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 for key in chunk['metadata'].keys():\n\n\u00a0 \u00a0 \u00a0 \u00a0 metadata[key] = chunk['metadata'][key]\n\n\n\n\u00a0 \u00a0 text = chunk['content']\n\n\u00a0 \u00a0 phishing_documents.append(Document(text=text,metadata=metadata))\n\n\n\n# Create an index\n\n index _retriever_openai = load_vector_index(\n\n \u00a0 \u00a0 \"vector_indexes/phishing_index_documents_openai\",\n\n \u00a0 \u00a0 OpenAIEmbedding(model=\"text-embedding-3-large\") \n\n )\n\n\n\nindex = VectorStoreIndex.from_documents(\n\n\u00a0 \u00a0 documents=phishing_documents,\n\n\u00a0 \u00a0 embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\")\n\n)\n\n\n\n\n\nQuery Engine: The created index can be utilized in two ways: 1) as a retriever to fetch relevant documents from the index, and 2) as a query engine, which combines retrieval and answer generation into a single process. The query engine can take multiple parameters that control both the documents retrieved from the index based on the query and the output generated. Internally, the query engine uses OpenAI's API to respond to user queries. Below are some of the parameters described:\n\nSimilarity_top_k: Number of top-k similar nodes to retrieve from the from the Vector Store for answering a specific query. Filters: These filters are used in order to search through the metadata of the documents in order to narrow the search space. Rerank: Whether to use a reranker to reorder the top-k results. Response_mode: When the Llama index is used as query engine we can define how the retrieved documents are synthesized in order to create a final answer that can be passed to the user. Options provided are the following:\n\n\u201crefine\u201d: creates and refines an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per retrieved chunk\n\n\u201ccompact\u201d: This is similar to \u201crefine\u201d mode but compact as the mode says. This concatenates the retrieved chunks, resulting in less LLM calls. \u201ctree summarize\u201d: The retrieved chunks are queried through a summary template on the LLM model, resulting in multiple answers. These answers are recursively used as chunks in a tree_summarize LLM call and so on, until there\u2019s only one chunk left, and thus only one final answer. \u201csimple_summarize\u201d: This truncates the retrieved chunks into a single LLM prompt. Good for quick summarization purposes, but may lose detail due to truncation\n\n\u201cno_text\u201d: This only returns the retrieved chunks without sending them to the LLM. Node_Postprocessors: Optional postprocessors can be applied to the retrieved nodes before the response synthesis step. Postprocessors can be used to filter out nodes based on their similarity, specific keywords. Postprocessors can also be used to introduce further reranking strategies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e612114-61f4-47a6-ac59-409d9ed64a1f": {"__data__": {"id_": "5e612114-61f4-47a6-ac59-409d9ed64a1f", "embedding": null, "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6498b211-a87c-445d-9f6f-7d97469b1bb4", "node_type": "4", "metadata": {"id": "cybercrime_7", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "7", "lang": "en", "jurisdiction": "GR"}, "hash": "67ceee059b1be00e0ace3a315b612ee34281bc5e025f6c03cc9e4b2302fb7c82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Text_qa_template / refine_template: Here we can define prompt templates that can be used during response generation. query_engine = index.as_query_engine()\n\n\n\nImproving the Retrieval-Augmented Generation (RAG) Pipeline:\n\nWhen the number of documents or text chunks is relatively small, a Retrieval-Augmented Generation (RAG) model performs efficiently. However, as the corpus scales, retrieving truly relevant context becomes increasingly computationally intensive. This can lead to slower responses and inconsistencies, especially when answering complex or specific queries. To maintain accuracy and responsiveness, several enhancement strategies can be incorporated into the RAG workflow. Metadata Filtering: Metadata filtering improves retrieval precision by narrowing the search space using structured attributes associated with each document or chunk (e.g., topic, language, jurisdiction). This method reduces noise and computational overhead by ensuring that only contextually relevant documents are considered during retrieval \u2014 even before vector similarity is calculated. Dynamic metadata filters can be applied based on query classification, which can be performed using LLMs or domain-specific encoders such as Legal-BERT. The effectiveness of metadata filtering depends on the richness and granularity of the metadata assigned during document indexing. query = \"What are the legal consequences of phishing in Greece?\"\n\nmetadata_filter = {\"topic\": \"Greek Penal Code\"}\n\nresults = vectorstore.similarity_search(query, filter=metadata_filter)\n\n\n\nEmbedding Model Fine-tuning: Embedding models lie at the heart of dense retrieval in RAG systems. While pretrained models like bge, E5, and MiniLM provide strong zero-shot performance, they often lack domain-specific nuance. Fine-tuning these models on curated (query, positive/negative context) examples significantly improves semantic alignment, especially in specialized domains such as law, healthcare, or finance. Training strategies may include:\n\nEmbedding training in a supervised manner by providing queries with corresponding positive and/or negative contexts to better distinguish semantic relationships. Unsupervised domain adaptation using in-domain corpora\n\nInstruction tuning, where structured prompts are used during fine-tuning to enhance intent encoding\n\nReranker [5]: As previously mentioned, we can input a reranker model as a parameter in the query engine. This step occurs after the initial retrieval phase and reorders the documents based on a deeper semantic understanding. Typically implemented as a cross-encoder (e.g., BERT, RoBERTa, LegalBERT), the reranker receives all initially retrieved documents are paired with the query, and a relevance score is computed using full cross-attention. Based on these scores, the top-k retrieved documents are reranked. This process effectively reduces hallucinations, filters out tangentially related documents, and ensures that only the most relevant and context-aligned chunks are passed to the LLM for generation. from sentence_transformers import CrossEncoder\n\n\n\ncandidate_docs = vector_db.query(\n\n    vector=query_embedding,\n\n    top_k=20,\n\n)\n\n\n\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n\n\ndef rerank(query, docs, top_k=5):\n\n    pairs = [(query, doc[\"text\"]) for doc in docs]\n\n    scores = reranker.predict(pairs)\n\n    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n\n    return [doc for doc, _ in reranked[:top_k]]\n\n\n\ntop_docs = rerank(query, candidate_docs)\n\n\n\nReranker Fine-tuning: Off-the-shelf reranker models are trained on general datasets and may not have specific knowledge of domain-specific data. Fine-tuning reranker models using domain-specific relevance judgments or synthetic approximations can drastically improve the reranking of the originally retrieved documents.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5391525c-be6b-487f-89fe-d0ff7f4fe3d6": {"__data__": {"id_": "5391525c-be6b-487f-89fe-d0ff7f4fe3d6", "embedding": null, "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5878b409-641a-459e-93df-f2b0da06c6d6", "node_type": "4", "metadata": {"id": "cybercrime_8", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "8", "lang": "en", "jurisdiction": "GR"}, "hash": "e2d250d7b3f0fd3b3a5e2d64d3aa795812be9cf4f147d442ef76fe5419164f5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this process, the data should be formatted as pairs, consisting of a query and a list of relevant passages or triplets that include negative passages or irrelevant documents. Platforms like Cohere also provide reranker models and reranker fine-tuning APIs (e.g., rerank-english-v2.0), allowing for tighter control over domain adaptation. Cohere: Cohere [6] is a powerful tool that offers applications based on large language models (LLMs) with a focus on enterprise use cases. Their models are optimized for multi-turn dialogue, summarization, classification tasks, and retrieval-augmented generation (RAG). Cohere provides both hosted APIs and on-premise deployment options, making it an attractive choice for scalable and secure chatbot applications. The tool is designed to be easily integrated into any custom application. Developers can access its capabilities via RESTful APIs or SDKs, such as the Cohere Python SDK, making it simple to generate AI completions, embeddings, and text classifications. The models support instruction-following tasks and are optimized for grounding in external contexts, which is essential for domain-specific applications such as legal, healthcare, and text documentation. One of Cohere's key strengths is its support for RAG-native workflows, as it can be used for both document retrieval and text generation. For retrieval purposes, it provides high-quality embedding models that support English text as well as multilingual text, including Greek (embed-v4.0). These embeddings can be stored in a vector database and utilized for semantic searches, similar to Llama-Index. Additionally, Cohere offers reranker models, such as rerank-english-v2.0, which can reorder retrieved documents based on their relevance to a user query, thus improving the quality of the final answer provided by the model. Furthermore, Cohere implements Command R models that are RAG-native, meaning they are designed to generate answers based on the text chunks retrieved based on user queries. These features make Cohere a strong backend for LLM-based search and question-answering systems, where accurate retrieval and reasoning generation are crucial. Cohere can also be integrated into the Llama-Index structure. Within LlamaIndex, Cohere can serve as a language model backend for generation, an embedding model for document indexing and retrieval, or a reranker to refine search results. This modular integration makes it easy to build and optimize custom RAG pipelines using Cohere's tools. Additionally, Cohere also offers the capability for model fine-tuning, allowing you to teach a model a new task or leverage new data. Fine-tuning within Cohere can be applied to specific tasks, such as creating chat conversations to enable chatbot-like behavior, text classification, and the reranking of documents. This is particularly useful for Retrieval-Augmented Generation (RAG).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14f956bd-0f64-4be6-a1f1-fbb327b04c50": {"__data__": {"id_": "14f956bd-0f64-4be6-a1f1-fbb327b04c50", "embedding": null, "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e62eccb5-333d-459c-90bc-dfa6f1789c1a", "node_type": "4", "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "hash": "968bad5299ba141e01344e2606cd6160472748599c243ba39d30f99a0c3eb245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d268625d-2c6a-4b29-8f78-1ccee89c7f26", "node_type": "1", "metadata": {}, "hash": "423fa17693c785a8b0ca09b4c64e7637bbc01ca8fe35c228baee9337359778e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To fine-tune a model for RAG, you need to provide a dataset that includes queries alongside relevant passages. This dataset should contain a list of documents or passages that answer the queries, as well as \"hard negatives\"\u2014examples that seem relevant to the query but do not actually contain the correct answer. The dataset should be at least 256 queries long. During the fine-tuning process, Cohere provides various metrics to evaluate effectiveness, including Accuracy@1, Accuracy@3, MMR@10, and nDCG@10. While direct fine-tuning of Cohere's proprietary models is not generally available to all users due to limited access in the free edition, there is still considerable potential to utilize Cohere effectively to enhance the Llama-Index structure for RAG applications. This allows developers to customize the system's behavior to meet specific domains or user needs, making it particularly effective for applications like legal chatbots, where accuracy and interpretability are critical. For the creation a finetuned reranker via Cohere, the data should be structured in JSON Lines format. Here are examples of how each line in the .jsonl\u00a0file should look:\n\n\n\nWithout Hard Negatives:\n\n{\n\n\u00a0 \u00a0 \"query\": \"What are the different types of phishing attacks?\u201d,\n\n\u00a0 \u00a0 \"relevant_passages\": [\"The different types of phishing attacks mentioned in the context are Email Phishing/Smishing, Spear Phishing, Social Media Phishing, and Vishing (voice phishing).\u201d]\n\n}\n\n\n\nWith Hard Negatives:\n\n{\n\n\u00a0 \"query\": \"What is the capital of France?\",\n\n\u00a0 \"relevant_passages\": [\"The different types of phishing attacks mentioned in the context are Email Phishing/Smishing, Spear Phishing, Social Media Phishing, and Vishing (voice phishing).\"],\n\n\u00a0 \"hard_negatives\": [\"Malware (Malicious Software) is not considered a phishing attack.\"]\n\n}\n\n\n\nEach JSON object should contain a\u00a0query, a list of\u00a0relevant_passages\u00a0that answer the query, and optionally, a list of\u00a0hard_negatives\u00a0that do not answer the query. Including hard negatives can improve the model's ability to distinguish between relevant and irrelevant information. After that we can upload it to the Cohere website and the finetuning begins. To use it we can do the following:\n\n\n\nImport cohere\n\n\n\nCOHERE_API_KEY = \"\" \u00a0# Replace with your Cohere API key\n\nco = cohere.ClientV2(COHERE_API_KEY)\n\nft = co.finetuning.get_finetuned_model(\u2018model_id\u2019)\n\n\n\nresponse = co.rerank(\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 query=query,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 documents=documents_texts,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model=cohere_model.finetuned_model.id + \"-ft\",\n\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\n\n\nHybrid Retrieval: Hybrid retrieval combines the strengths of dense (vector-based) and sparse (keyword-based) retrieval methods to maximize both semantic relevance and exact keyword matching. This approach is particularly effective in domains such as law and cybersecurity, where important concepts may not be semantically similar but are lexically precise. For dense retrieval, embedding-based models like BGE or E5, or fine-tuned domain-specific models, can be used. For sparse retrieval, traditional keyword-based searches using techniques like TF-IDF, BM25, or inverted indexes can be employed. Fusion methods (e.g., reciprocal rank fusion, weighted scoring) ensure that results are not only topically aligned but also lexically accurate. from rank_bm25 import BM25Okapi\n\n\n\n### Dense Retrieval\n\n\n\ndense_results = dense_vector_db.query(\n\n    vector=query_embedding,\n\n    top_k=10\n\n)\n\n\n\n### Sparse Retrieval\n\n\n\ntokenized_corpus = [doc[\"text\"].split() for doc in all_documents]\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n\n\nsparse_scores = bm25.get_scores(query.split())\n\ntop_sparse_indices = sorted(range(len(sparse_scores)), key=lambda i: sparse_scores[i], reverse=True)[:10]\n\nsparse_results = [all_documents[i] for i in top_sparse_indices]\n\n\n\n### Merge Results\n\n\n\ndef hybrid_merge(dense, sparse):\n\n    seen = set()\n\n    merged = []\n\n    for doc in dense + sparse:\n\n        doc_id = doc.get(\"id\") or doc[\"text\"]  # Simple dedup\n\n        if doc_id not in seen:\n\n            merged.append(doc)\n\n            seen.add(doc_id)\n\n    return merged[:15]  # or any top-k\n\n\n\nhybrid_docs = hybrid_merge(dense_results, sparse_results)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d268625d-2c6a-4b29-8f78-1ccee89c7f26": {"__data__": {"id_": "d268625d-2c6a-4b29-8f78-1ccee89c7f26", "embedding": null, "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e62eccb5-333d-459c-90bc-dfa6f1789c1a", "node_type": "4", "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "hash": "968bad5299ba141e01344e2606cd6160472748599c243ba39d30f99a0c3eb245", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14f956bd-0f64-4be6-a1f1-fbb327b04c50", "node_type": "1", "metadata": {"id": "cybercrime_9", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "9", "lang": "en", "jurisdiction": "GR"}, "hash": "c507ca281937f56052bed78f6b644f5c07799bcd5d9156ed780ddf5a50bb902a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Sparse Retrieval\n\n\n\ntokenized_corpus = [doc[\"text\"].split() for doc in all_documents]\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n\n\nsparse_scores = bm25.get_scores(query.split())\n\ntop_sparse_indices = sorted(range(len(sparse_scores)), key=lambda i: sparse_scores[i], reverse=True)[:10]\n\nsparse_results = [all_documents[i] for i in top_sparse_indices]\n\n\n\n### Merge Results\n\n\n\ndef hybrid_merge(dense, sparse):\n\n    seen = set()\n\n    merged = []\n\n    for doc in dense + sparse:\n\n        doc_id = doc.get(\"id\") or doc[\"text\"]  # Simple dedup\n\n        if doc_id not in seen:\n\n            merged.append(doc)\n\n            seen.add(doc_id)\n\n    return merged[:15]  # or any top-k\n\n\n\nhybrid_docs = hybrid_merge(dense_results, sparse_results)\n\n\n\n\n\nMulti-Query Expansion: Multi-query expansion enhances recall by generating semantically diverse variants of the original user query. It is a technique that transforms a single user query into multiple semantically diverse sub-queries. Each variation reflects a slightly different intent, framing, or lexical formulation of the original prompt. These variations are processed in parallel to retrieve a broader and more relevant document set. This can be achieved with:\n\nLarge Language Models (LLMs) by prompting them to rephrase the user query, or \n\nBy using paraphrase models like T5 or Pegasus.", "mimetype": "text/plain", "start_char_idx": 3428, "end_char_idx": 4758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e9d5024-170f-40d2-8558-390e341ef828": {"__data__": {"id_": "3e9d5024-170f-40d2-8558-390e341ef828", "embedding": null, "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86d35ee7-a9e0-4e84-8f1d-03285f519f19", "node_type": "4", "metadata": {"id": "cybercrime_10", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "10", "lang": "en", "jurisdiction": "GR"}, "hash": "a58f32f554b8b47a718f3231b9e2f6a584dfbdd2f37c63ca498fee4624ddbc82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each query variant is embedded and processed independently. Retrieved results are merged, deduplicated, and reranked before being passed to the LLM. This method can expand recall coverage, especially for under-specified or ambiguous queries, surface relevant content with varied lexical overlaps, and trigger metadata filters more effectively. from transformers import pipeline\n\n\n\n### Expansion of Query\n\n\n\ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n\n\n\ndef expand_query(query, num=3):\n\n    prompt = f\"Generate {num} rephrasings of the following legal question:\\n'{query}'\"\n\n    output = generator(prompt, max_new_tokens=64)[0]['generated_text']\n\n    return [q.strip() for q in output.split('\\n') if q.strip()]\n\n    \n\nexpanded_queries = expand_query(\"What are the penalties for data breaches under Greek law?\")\n\n\n\n### Retrieve for each expanded Query\n\n\n\nall_results = []\n\nfor eq in expanded_queries:\n\n    eq_emb = embedder.encode(eq)\n\n    results = vector_db.query(vector=eq_emb, top_k=5)\n\n    all_results.extend(results)\n\n\n\n### Merge and deduplicate documents\n\n\n\ndef dedupe(docs):\n\n    seen = set()\n\n    merged = []\n\n    for doc in docs:\n\n        key = doc.get(\"id\") or doc[\"text\"]\n\n        if key not in seen:\n\n            merged.append(doc)\n\n            seen.add(key)\n\n    return merged\n\n\n\nunique_docs = dedupe(all_results)\n\n\n\n\n\nFeedback Loop Reinforcement: To enhance the long-term performance of the model, a feedback loop mechanism can be integrated into the retrieval-augmented generation (RAG) framework. This component allows the system to learn from its interactions with users by collecting and utilizing explicit and implicit feedback signals, thus improving the retrieval and generation process over time. This dynamic process enables the system to adapt to domain shifts, user expectations, and previous retrieval errors, resulting in more accurate and contextually aligned responses. User feedback can include thumbs up/down ratings on generated answers, selection of preferred documents for specific queries, or user engagement patterns, such as continuing the dialogue or disengaging after a single response, which can indicate satisfaction with previous results. Additionally, legal experts may manually review answers and provide correctness labels. This feedback can be integrated into various components of the RAG system, from further fine-tuning the embedding model and retriever to the reranker and answer generation component. While improvements to the internal components of the RAG system cannot occur in real-time, user feedback can be used to enhance answer generation. Evaluate and Iterate:\n\nAn effective strategy is to evaluate the model's performance on a validation set after each integration step, ensuring that changes lead to measurable improvements. This iterative approach allows us to leverage the strengths of RAG-based retrieval while refining the system\u2019s performance for our specific data and legal use cases\n\n\n\nThe processes outlined above can be orchestrated within an agentic workflow using LangChain [7]- a powerful framework designed to construct intelligent, modular pipelines built around LLM-driven agents. LangChain enables each component of the RAG system to be handled by specialized agents, each with distinct roles and responsibilities. Unlike traditional static workflows, LangChain\u2019s agentic model introduces dynamic reasoning, self-reflection, and decision-making, allowing the system to adapt in real-time to ambiguous queries, multilingual inputs, and evolving contexts. This flexibility empowers the pipeline to respond with both precision and resilience, even in complex legal or multilingual domains. SECTION C: Alternative RAG approaches\n\nAs the field of Retrieval-Augmented Generation (RAG) continues to evolve, more advanced and modular pipeline architectures are being developed to address real-world scenarios in the legal, biomedical, enterprise, and multilingual domains. While frameworks like LangChain and Haystack[8] provide a solid foundation for basic RAG implementations, certain use cases require specialized architectures or systems that go beyond the scope of using vector indexes and similarity for document retrieval. GraphRAG\n\nGraphRAG [9] integrates structured knowledge representations \u2013 such as ontologies, entity-relation graphs, and RDF triples - into the retrieval pipeline. When analyzing documents, agents identify semantic relationships and attempt to construct a knowledge graph from the corpus. This transformation makes retrieval a semantic traversal task rather than relying solely on vector similarity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6397f60-6c77-4444-8b57-add3acc9dc55": {"__data__": {"id_": "b6397f60-6c77-4444-8b57-add3acc9dc55", "embedding": null, "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9959a8a-b143-47fd-8a92-8509de87b055", "node_type": "4", "metadata": {"id": "cybercrime_11", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "11", "lang": "en", "jurisdiction": "GR"}, "hash": "eedd110535ecdbdf93befdcfc811fa55d7125006348aa48a258fbfc29bd9fe88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of depending on unstructured document chunks, the system queries a graph-based representation of knowledge to extract or infer relevant entities, facts, and logical constraints. This approach is particularly beneficial in regulated domains like law or healthcare, where factual consistency and explainability are crucial. However, GraphRAG faces challenges in terms of maintainability and scalability; adding new documents often requires partial or complete regeneration of the graph structure, which can become a bottleneck in rapidly changing corpora or streaming data environments. Techniques such as incremental graph construction and knowledge graph pruning can be explored to address these limitations. Figure 4 Knowledge Graph Pipeline for creating the graph and its use when user inputs a query [10]. LightRAG\n\nLightRAG [11] is a streamlined variant of the RAG pipeline designed for environments where computational efficiency is critical, such as mobile deployments, edge computing, or real-time applications. It eliminates optional components, like rerankers, metadata filters, or multi-index routing, to simplify retrieval and response generation. While this might reduce answer precision in certain domains, it results in faster inference, lower power consumption, and significantly reduced infrastructure costs. LightRAG is especially advantageous in high-throughput settings, including customer support bots, embedded legal advisors, and multilingual FAQ systems\u2014where response latency must be tightly controlled. AgenticRAG\n\nAgentic RAG [12] introduces autonomous agents into the RAG workflow, allowing LLM-powered agents to make decisions about which tools to invoke, including retrieval, reasoning, summarization, and external APIs. In this model, retrieval is not a fixed step; it is one of many tools available in an agent\u2019s toolbox. Agents dynamically determine whether retrieval is necessary, how to phrase the query, whether to reformulate it, and how to combine results from multiple sources. This architecture promotes adaptive decision-making, complex multi-step workflows, and intelligent fallback strategies, making it particularly powerful in legal research, case triage, and compliance pipelines. When combined with agent orchestration frameworks like LangGraph or CrewAI, Agentic RAG evolves into an intelligent knowledge worker. RAGFlow\n\nRAGFlow [13] emphasizes the data preparation layer, which is often overlooked but essential for effective RAG performance. It provides robust preprocessing, chunking, embedding, and routing mechanisms designed for high-volume enterprise data. RAGFlow can process heterogeneous inputs, such as scanned contracts, spreadsheets, PDFs, and multilingual documents, into structured, retrievable vector formats. It supports conditional chunking strategies\u2014like title-aware, table-sensitive, and legal clause-based chunking\u2014and pipeline parallelization to scale across hundreds of thousands of documents. Its modularity makes it well-suited for long document RAG, contract analysis, medical record processing, and enterprise search. When paired with hybrid retrieval and re-ranking, RAGFlow ensures both high recall and semantic precision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69cee649-a768-40bd-8e3d-d0ff9ea296d4": {"__data__": {"id_": "69cee649-a768-40bd-8e3d-d0ff9ea296d4", "embedding": null, "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3063718-9644-472b-8f59-d7dd1b67ef37", "node_type": "4", "metadata": {"id": "cybercrime_12", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "12", "lang": "en", "jurisdiction": "GR"}, "hash": "27cf414f7c73ab11cfb763850ea2b19ba35f7d0a327ffc049da2bc40b6dff2bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cache-Augmented Generation (CAG)\n\nIn a classic RAG pipeline, the system first pulls top-ranked documents or text chunks, inputs these retrieved chunks into the user\u2019s query and a Large Language Model processes the augmented prompt to produce the final output. However, each query triggers document retrieval, which can slow down responses and there is a possibility of selecting irrelevant or outdated documents which means that the output suffers. In contrast, Cache-Augmented Generation (CAG) [14] reverses this paradigm. In CAG, relevant context is preloaded into the extended context window of a large model (e.g., via a sliding cache or prompt streaming), and its attention weights or KV cache are retained throughout a session. This approach eliminates the need for runtime retrieval, enabling zero-latency, low-drift generation with cached semantic context. CAG is particularly beneficial for high-interaction sessions (e.g., legal advisory bots or code assistants) where topics evolve slowly over time, allowing cached documents to be utilized for multiple queries without degrading performance. To avoid context overflow, CAG systems often implement smart cache invalidation and chunk management strategies. However, there are some trade-offs, with the most important being the high cost of tokens. Since the entire cached context\u2014often tens of thousands of tokens\u2014is processed with each generation, the costs for input tokens can become exorbitant, especially with commercially priced language models like GPT-4 or Claude. This makes CAG potentially cost-prohibitive on a large scale, unless it is paired with aggressive summarization, token pruning, or semantic compression techniques. Another limitation of CAG is context staleness. As the cached knowledge remains static during a session, any updates, corrections, or deletions made to the source documents won't be reflected unless the cache is explicitly invalidated and refreshed. This process reintroduces the retrieval step and its associated latency, which can be problematic in fields where document updates are frequent or critical for legal or factual accuracy. Moreover, even with the introduction of 128K or 1M-token context windows, there are practical limits on how much content can be cached, particularly when considering memory or performance constraints. To maintain efficiency and relevance, intelligent cache management strategies\u2014such as semantic deduplication, importance-weighted chunk prioritization, and adaptive window shifting\u2014are often necessary. CONCLUSIONS\n\nIn this deliverable, we provided a comprehensive overview of how Retrieval-Augmented Generation (RAG) works. We detailed our implementation of RAG in the AILA framework to develop agents that effectively provide information on phishing. Specifically, we described a RAG pipeline that encompasses data ingestion, querying, and document retrieval. Additionally, we discussed several enhancements that can be made to improve RAG performance, such as introducing rerankers, applying metadata filtering, and fine-tuning embeddings. Lastly, we shared information about alternative strategies that resemble RAG and can be used as substitutes for a straightforward RAG pipeline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7dca46f9-1f95-4c58-863b-e80638089423": {"__data__": {"id_": "7dca46f9-1f95-4c58-863b-e80638089423", "embedding": null, "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "191ca079-b4fb-4a2a-870b-2853bcc34aeb", "node_type": "4", "metadata": {"id": "cybercrime_13", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "13", "lang": "en", "jurisdiction": "GR"}, "hash": "93c4d07128710354dcd57429b829091c0173605e70616352144f85096440625d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "REFERENCES\u00a0 \n\n[1] Douze, Matthijs, Alexandr Guzhva, Chengqi Deng, et al. \u201cThe Faiss Library.\u201d Version 3. Preprint, arXiv, 2024. https://doi.org/10.48550/ARXIV.2401.08281. [2] https://docs.trychroma.com/docs/overview/introduction\n\n[3] https://huggingface.co/datasets/umarbutler/open-australian-legal-embeddings\n\n[4] https://huggingface.co/nlpaueb/legal-bert-base-uncased\n\n[5] X. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero\u2011Shot Listwise Document Reranking with a Large Language Model,\u201d CoRR, vol. abs/2305.02156, May\u202f2023. doi:\u202f10.48550/arXiv.2305.02156\n\n[6] https://cohere.com/\n\n[7] Topsakal, O., & Akinci, T. C. (2023, July). Creating large language model applications utilizing langchain: A primer on developing llm apps fast. In\u00a0International conference on applied engineering and natural sciences\u00a0(Vol. 1, No. 1, pp. 1050-1056). [8] https://haystack.deepset.ai\n\n[9] Han, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y., ... & Tang, J.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9338331-0bdb-4d20-bfd6-687f10ac360a": {"__data__": {"id_": "e9338331-0bdb-4d20-bfd6-687f10ac360a", "embedding": null, "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c31ddb12-4781-436d-bf48-95832d54aadc", "node_type": "4", "metadata": {"id": "cybercrime_14", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "14", "lang": "en", "jurisdiction": "GR"}, "hash": "62c66717b5b75a377ba40bc69e169b21af7737d7893386698a8a28d150c78044", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2024). Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309. [10] https://www.nebula-graph.io/posts/graph-RAG\n\n[11] Guo, Z., Xia, L., Yu, Y., Ao, T., & Huang, C. (2024). Lightrag: Simple and fast retrieval-augmented generation. arXiv preprint arXiv:2410.05779. [12] Singh, A., Ehtesham, A., Kumar, S., & Khoei, T. T. (2025). Agentic retrieval-augmented generation: A survey on agentic rag. arXiv preprint arXiv:2501.09136. [13] https://github.com/infiniflow/ragflow\n\n[14] Chan, B. J., Chen, C. T., Cheng, J. H., & Huang, H. H. (2025, May). Don't do rag: When cache-augmented generation is all you need for knowledge tasks. In\u00a0Companion Proceedings of the ACM on Web Conference 2025\u00a0(pp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1b7f119-65ba-4834-a82f-b882afc728aa": {"__data__": {"id_": "f1b7f119-65ba-4834-a82f-b882afc728aa", "embedding": null, "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0eb4717-c10d-4331-a5e7-a4d0013fb213", "node_type": "4", "metadata": {"id": "cybercrime_15", "title": "HFRI-AILA-15440-Deliverable2.3C final.docx", "source": "Greek Cybercrime Law", "doc_type": "criminal_statute", "law": "unknown", "article_number": "15", "lang": "en", "jurisdiction": "GR"}, "hash": "67f086fa9f2e92320d9822d43d4b6dace7265393097a16206972df22245a4c20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "893-897).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 9, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}